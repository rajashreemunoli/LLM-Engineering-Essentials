{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nebius-Academy/LLM-Engineering-Essentials/blob/main/topic3/3.2_database_search_and_vector_stores.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM Engineering Essentials by Nebius Academy\n",
        "\n",
        "Course github: [link](https://github.com/Nebius-Academy/LLM-Engineering-Essentials/tree/main)\n",
        "\n",
        "The course is in development now, with more materials coming soon."
      ],
      "metadata": {
        "id": "tC_aK78vqn_P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM Engineering Essentials 3.2. Database search and Vector Stores\n",
        "\n",
        "In the previous notebook we used web search to provide additional context to LLMs, but often the relevant information doesn't just lie out there in the web - instead, it's contained in your company's internal databases.\n",
        "\n",
        "In this notebook, you'll learn how to implement RAG with two popular database types: relational databases and vector stores. Another option - graph databases - will be considered in the next notebook.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1d4fUfWEYn6X1XW-B5VJNy5e5DZxZ30CC\" width=600 />\n",
        "</center>\n",
        "\n",
        "**Note**. We'll still be using LLM APIs, but in many real-world situations this won't be a good option, because you don't want to expose your internal data to a third-party API. In such cases, self-hosted LLMs come to help; we'll learn how to use them further in the course."
      ],
      "metadata": {
        "id": "oKXTtr5gUm7q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting things ready"
      ],
      "metadata": {
        "id": "P8-i76LhVsqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q openai"
      ],
      "metadata": {
        "id": "WqCgRtIRIcN3"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "with open(\"nebius_api_key\", \"r\") as file:\n",
        "    nebius_api_key = file.read().strip()\n",
        "\n",
        "os.environ[\"NEBIUS_API_KEY\"] = nebius_api_key"
      ],
      "metadata": {
        "id": "NRpRGdl5IdJZ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Database search\n",
        "\n",
        "In many applications, you need to supply the LLM with information about different product technicalities, which are stored in some internal database.\n",
        "\n"
      ],
      "metadata": {
        "id": "KdXCnsLbGzZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1: Text-to-SQL\n",
        "\n",
        "In in many cases, the data source would be a **relational database**. Then, the **query tool** will have to reformulate the initial user's request as an SQL-like query. An example might be:\n",
        "\n",
        "**User's request**: `Er, I'd like buy some potions, but I'm low on cash. What are the most affordable options?`\n",
        "\n",
        "**SQL query**:\n",
        "\n",
        "```sql\n",
        "SELECT item_name, price\n",
        "FROM shop_inventory\n",
        "WHERE category = 'potion'\n",
        "ORDER BY price ASC;\n",
        "```\n",
        "\n",
        "The task of generation of SQL queries from users' requests is known as **Text-to-SQL**. It's not an easy one! If you want to check how good LLMs and LLM-based systems are at it, we recomment to check the leaderboards of [BIRD-SQL Big Bench](https://bird-bench.github.io/) and [BIRD-CRITIC](https://bird-critic.github.io/). The second one is harder and only scores pure LLMs, not LLM-powered system, and it shows quite sad results; but even for BIRD-SQL AI systems are below 80% accuracy.\n",
        "\n",
        "To compensate for this weakness, Text-to-SQL systems often introduce validating and debugging chains that improve the initial query until it's working. Here's, for example, a schematics describing the [Open Data QnA](https://github.com/GoogleCloudPlatform/Open_Data_QnA) framework by Google Cloud Plaftorm\n",
        "\n",
        "<center>\n",
        "<img src=\"https://raw.githubusercontent.com/GoogleCloudPlatform/Open_Data_QnA/refs/heads/main/utilities/imgs/OpenDataQnA_architecture.png\" width=800 />\n",
        "\n",
        "[Source](https://github.com/GoogleCloudPlatform/Open_Data_QnA)\n",
        "</center>\n",
        "\n",
        "We recommend you to check several top solutions from the BIRD leaderboard and take note of their system prompts. We'll only give general advise here. A typical Text-to-SQL prompt contains:\n",
        "\n",
        "* A set of instructions, the more detailed, the better. Some instructions may explain the nature of the databases or some typical use cases; others set **restrictions**, usually forbidding the LLM from generating table-changing queries (INSERT, UPDATE, DELETE, DROP etc).\n",
        "\n",
        "  Please note that even despite these restrictions, you should have additional post-LLM checks for that (based on regular expressions or other LLMs) to avoid your tables getting corrupted as a result of a hallucination or a jailbreal.\n",
        "\n",
        "* Database schemas of all the necessary tables.\n",
        "\n",
        "* If you can predict some frequent use cases beforehand, showing them in a system prompt won't also hurt.\n",
        "\n",
        "Let's check an example!"
      ],
      "metadata": {
        "id": "0kruq0vyR5U-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## An example: a Potion Shop helper bot\n",
        "\n",
        "We'll work with a simple SQL database, that has three tables:\n",
        "\n",
        "```sql\n",
        "CREATE TABLE shop_inventory (\n",
        "  potion_id INTEGER PRIMARY KEY,      -- Unique ID of the potion\n",
        "  stock INTEGER,                      -- How many are in stock\n",
        "  price INTEGER                       -- Price in gold\n",
        ")\n",
        "\n",
        "CREATE TABLE potions (\n",
        "  potion_id INTEGER PRIMARY KEY,      -- Matches inventory ID\n",
        "  potion_name TEXT,                   -- Name of the potion\n",
        "  category TEXT,                      -- Category (healing, mana, etc.)\n",
        "  effect TEXT,                        -- Specific effect (heals 10 hp, etc.)\n",
        "  rarity TEXT,                        -- common, uncommon, rare, legendary\n",
        "  duration TEXT,                      -- How long it lasts (e.g., '1 min', '10 min', 'permanent')\n",
        "  side_effects TEXT                   -- Possible side effects (nullable)\n",
        ")\n",
        "\n",
        "CREATE TABLE purchases (\n",
        "  purchase_id INTEGER PRIMARY KEY,\n",
        "  customer_name TEXT,                 -- Name of the customer\n",
        "  potion_id INTEGER,                  -- Purchased potion\n",
        "  quantity INTEGER,                   -- Number bought\n",
        "  date DATE,                          -- Date of purchase\n",
        "  FOREIGN KEY(potion_id) REFERENCES potions(potion_id)\n",
        ")\n",
        "```\n",
        "\n",
        "The following code will download these databases along with some useful loader and query scripts."
      ],
      "metadata": {
        "id": "l35D_jWQhTRw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/Nebius-Academy/LLM-Engineering-Essentials/raw/main/topic3/potion_shop.db -O potion_shop.db\n",
        "!wget https://github.com/Nebius-Academy/LLM-Engineering-Essentials/raw/main/topic3/potion_shop_utils.py -O potion_shop_utils.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7iczD-itgebA",
        "outputId": "a39722d5-a5ed-467d-80e3-f281ff6f1d80"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-23 01:56:23--  https://github.com/Nebius-Academy/LLM-Engineering-Essentials/raw/main/topic3/potion_shop.db\n",
            "Resolving github.com (github.com)... 140.82.121.3\n",
            "Connecting to github.com (github.com)|140.82.121.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/Nebius-Academy/LLM-Engineering-Essentials/main/topic3/potion_shop.db [following]\n",
            "--2025-04-23 01:56:24--  https://raw.githubusercontent.com/Nebius-Academy/LLM-Engineering-Essentials/main/topic3/potion_shop.db\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 16384 (16K) [application/octet-stream]\n",
            "Saving to: ‘potion_shop.db’\n",
            "\n",
            "potion_shop.db      100%[===================>]  16.00K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-04-23 01:56:24 (74.1 MB/s) - ‘potion_shop.db’ saved [16384/16384]\n",
            "\n",
            "--2025-04-23 01:56:24--  https://github.com/Nebius-Academy/LLM-Engineering-Essentials/raw/main/topic3/potion_shop_utils.py\n",
            "Resolving github.com (github.com)... 140.82.121.3\n",
            "Connecting to github.com (github.com)|140.82.121.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/Nebius-Academy/LLM-Engineering-Essentials/main/topic3/potion_shop_utils.py [following]\n",
            "--2025-04-23 01:56:24--  https://raw.githubusercontent.com/Nebius-Academy/LLM-Engineering-Essentials/main/topic3/potion_shop_utils.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8863 (8.7K) [text/plain]\n",
            "Saving to: ‘potion_shop_utils.py’\n",
            "\n",
            "potion_shop_utils.p 100%[===================>]   8.66K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-04-23 01:56:24 (101 MB/s) - ‘potion_shop_utils.py’ saved [8863/8863]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's load the database and run an SQL query to get three most expensive potions:"
      ],
      "metadata": {
        "id": "PC5Cqg3eh1ZF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from potion_shop_utils import (\n",
        "    create_potion_shop_database,\n",
        "    show_schema,\n",
        "    show_table,\n",
        "    query_db\n",
        ")\n",
        "\n",
        "# Create the database\n",
        "conn = create_potion_shop_database()\n",
        "\n",
        "# Display schema\n",
        "show_schema(conn)\n",
        "\n",
        "# Display all tables\n",
        "show_table(conn, \"potions\")\n",
        "show_table(conn, \"shop_inventory\")\n",
        "show_table(conn, \"purchases\")\n",
        "\n",
        "# Example query\n",
        "print(\"\\nExample query: Most expensive potions\")\n",
        "result = query_db(conn, \"\"\"\n",
        "SELECT p.potion_name, s.price\n",
        "FROM potions p\n",
        "JOIN shop_inventory s ON p.potion_id = s.potion_id\n",
        "ORDER BY s.price DESC\n",
        "LIMIT 3\n",
        "\"\"\")\n",
        "\n",
        "for name, price in result:\n",
        "    print(f\"{name}: {price} gold\")\n",
        "\n",
        "Close the connection\n",
        "conn.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6beQ86th0zT",
        "outputId": "2f2aea67-fcd6-4d43-8cf1-3c279adc1511"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Database already exists at potion_shop.db\n",
            "\n",
            "== Database Schema ==\n",
            "\n",
            "Table: shop_inventory\n",
            "  potion_id (INTEGER) [PRIMARY KEY]\n",
            "  stock (INTEGER)\n",
            "  price (INTEGER)\n",
            "\n",
            "Table: potions\n",
            "  potion_id (INTEGER) [PRIMARY KEY]\n",
            "  potion_name (TEXT)\n",
            "  category (TEXT)\n",
            "  effect (TEXT)\n",
            "  rarity (TEXT)\n",
            "  duration (TEXT)\n",
            "  side_effects (TEXT)\n",
            "\n",
            "Table: purchases\n",
            "  purchase_id (INTEGER) [PRIMARY KEY]\n",
            "  customer_name (TEXT)\n",
            "  potion_id (INTEGER)\n",
            "  quantity (INTEGER)\n",
            "  date (DATE)\n",
            "\n",
            "Table: potions\n",
            "    potion_id                potion_name    category  \\\n",
            "0           1       Minor Healing Potion     healing   \n",
            "1           2             Healing Potion     healing   \n",
            "2           3       Major Healing Potion     healing   \n",
            "3           4          Minor Mana Potion        mana   \n",
            "4           5                Mana Potion        mana   \n",
            "5           6          Major Mana Potion        mana   \n",
            "6           7      Crude Barkskin Potion  protection   \n",
            "7           8    Refined Barkskin Potion  protection   \n",
            "8           9         Swift Speed Potion    movement   \n",
            "9          10      Superior Speed Potion    movement   \n",
            "10         11  Frenzied Berserker Elixir      combat   \n",
            "11         12    Mighty Berserker Elixir      combat   \n",
            "12         13              Clarity Tonic      mental   \n",
            "13         14    Dreamless Sleep Draught        rest   \n",
            "14         15                   Antidote    curative   \n",
            "\n",
            "                                          effect    rarity duration  \\\n",
            "0                                    heals 10 hp    common  instant   \n",
            "1                                    heals 25 hp  uncommon  instant   \n",
            "2                                    heals 50 hp      rare  instant   \n",
            "3                                 restores 15 mp    common  instant   \n",
            "4                                 restores 30 mp  uncommon  instant   \n",
            "5                                 restores 60 mp      rare  instant   \n",
            "6                 reduces physical damage by 10%    common    5 min   \n",
            "7                 reduces physical damage by 25%      rare   10 min   \n",
            "8                increases movement speed by 30%  uncommon    3 min   \n",
            "9                increases movement speed by 50%      rare    5 min   \n",
            "10                increases attack damage by 25%  uncommon    2 min   \n",
            "11                increases attack damage by 50%      rare    3 min   \n",
            "12                 improves focus and perception  uncommon   10 min   \n",
            "13  provides 8 hours of restful sleep in 4 hours      rare  4 hours   \n",
            "14               cures common poisons and toxins  uncommon  instant   \n",
            "\n",
            "               side_effects  \n",
            "0                      none  \n",
            "1           mild drowsiness  \n",
            "2        temporary weakness  \n",
            "3                      none  \n",
            "4             mild headache  \n",
            "5       temporary brain fog  \n",
            "6              skin dryness  \n",
            "7          reduced mobility  \n",
            "8            mild dizziness  \n",
            "9   exhaustion after effect  \n",
            "10          reduced defense  \n",
            "11            tunnel vision  \n",
            "12         sensory overload  \n",
            "13   grogginess upon waking  \n",
            "14       stomach discomfort  \n",
            "\n",
            "Table: shop_inventory\n",
            "    potion_id  stock  price\n",
            "0           1     25     10\n",
            "1           2     15     25\n",
            "2           3      5     60\n",
            "3           4     20      8\n",
            "4           5     12     20\n",
            "5           6      6     45\n",
            "6           7     10     15\n",
            "7           8      4     40\n",
            "8           9      8     30\n",
            "9          10      3     65\n",
            "10         11      7     35\n",
            "11         12      2     70\n",
            "12         13      5     25\n",
            "13         14      3     50\n",
            "14         15     10     20\n",
            "\n",
            "Table: purchases\n",
            "    purchase_id customer_name  potion_id  quantity        date\n",
            "0             1        Alaric          4         3  2024-01-15\n",
            "1             2         Elara          5         1  2024-01-20\n",
            "2             3        Thorne          1         3  2024-01-25\n",
            "3             4          Lyra          7         2  2024-02-01\n",
            "4             5        Dorian          3         1  2024-02-05\n",
            "5             6         Elara          9         1  2024-02-10\n",
            "6             7        Thorne          6         2  2024-02-15\n",
            "7             8        Alaric         13         1  2024-02-20\n",
            "8             9          Lyra          1         2  2024-02-25\n",
            "9            10        Dorian          2         3  2024-03-01\n",
            "10           11        Thorne         12         1  2024-03-05\n",
            "11           12        Alaric          4         2  2024-03-10\n",
            "12           13         Seren         15         3  2024-03-15\n",
            "13           14         Seren          3         1  2024-03-20\n",
            "14           15          Lyra         10         1  2024-03-25\n",
            "15           16         Elara         14         1  2024-04-01\n",
            "16           17        Dorian          8         1  2024-04-05\n",
            "17           18        Thorne         11         2  2024-04-10\n",
            "18           19         Seren          5         3  2024-04-15\n",
            "19           20        Alaric          1         5  2024-04-20\n",
            "\n",
            "Example query: Most expensive potions\n",
            "Mighty Berserker Elixir: 70 gold\n",
            "Superior Speed Potion: 65 gold\n",
            "Major Healing Potion: 60 gold\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This seems correct.\n",
        "\n",
        "Now, we'll create a class `TextToSQLRAGBot` implementing a RAG-powered bot that for each user's request:\n",
        "\n",
        "* Creates an SQL query. For that, we use by default the **Phi-4** model my Microsoft, which is one of the top open source models at the [BIRD-CRITIC](https://bird-critic.github.io/) leaderboard.\n",
        "* Runs this SQL query.\n",
        "* Formulates a human-friendly answer based on the user's request and on whatever was retrieved from the database. For that, we use by default **Llama-3.1-70B**, but of course a much smaller model would work as well.\n",
        "\n",
        "We decided that a simple scenario doesn't require implementing multi-turn conversations, so the bot has no conversation memory.\n",
        "\n",
        "And first of all, let's create a system prompt for SLQ generation according to the guidelines discussed above. Note that it's not a system prompt in a strict sense, because the user's question will be a part of it as **{question}**:"
      ],
      "metadata": {
        "id": "kjcC_Yo5iIzM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_to_sql_system_prompt = \"\"\"**Task**\n",
        "\n",
        "Generate a SQL query to answer the user's question. The data comes from a fantasy role-playing game shop system.\n",
        "\n",
        "**Instructions**\n",
        "\n",
        "- Generate a syntactically correct SQL query using only the schema provided.\n",
        "- Do **not** select all columns with `SELECT *`; only select relevant columns.\n",
        "- If the user requests sorted results, use `ORDER BY`—otherwise, avoid it.\n",
        "- Do **not** return columns the user did not explicitly ask for.\n",
        "- If a question cannot be answered with the available schema, return: `'I do not know'`.\n",
        "- Do **not** use DML statements (INSERT, UPDATE, DELETE, DROP, etc.).\n",
        "- Always use meaningful table aliases when joining multiple tables.\n",
        "- You may assume `gold` is the unit of currency.\n",
        "\n",
        "**Database Schema**\n",
        "\n",
        "This query will run on a database with the following schema:\n",
        "\n",
        "```sql\n",
        "CREATE TABLE shop_inventory (\n",
        "  potion_id INTEGER PRIMARY KEY,      -- Unique ID of the potion\n",
        "  stock INTEGER,                      -- How many are in stock\n",
        "  price INTEGER                       -- Price in gold\n",
        ")\n",
        "\n",
        "CREATE TABLE potions (\n",
        "  potion_id INTEGER PRIMARY KEY,      -- Matches inventory ID\n",
        "  potion_name TEXT,                   -- Name of the potion\n",
        "  category TEXT,                      -- Category (healing, mana, etc.)\n",
        "  effect TEXT,                        -- Specific effect (heals 10 hp, etc.)\n",
        "  rarity TEXT,                        -- common, uncommon, rare, legendary\n",
        "  duration TEXT,                      -- How long it lasts (e.g., '1 min', '10 min', 'permanent')\n",
        "  side_effects TEXT                   -- Possible side effects (nullable)\n",
        ")\n",
        "\n",
        "CREATE TABLE purchases (\n",
        "  purchase_id INTEGER PRIMARY KEY,\n",
        "  customer_name TEXT,                 -- Name of the customer\n",
        "  potion_id INTEGER,                  -- Purchased potion\n",
        "  quantity INTEGER,                   -- Number bought\n",
        "  date DATE,                          -- Date of purchase\n",
        "  FOREIGN KEY(potion_id) REFERENCES potions(potion_id)\n",
        ")\n",
        "```\n",
        "\n",
        "**Your Output**\n",
        "\n",
        "Given the schema above, return the correct SQL query to answer this question:\n",
        "**‘{question}’**\n",
        "\n",
        "```sql\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "nboEXZyzgvDd"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Dict, Any, Optional, Callable\n",
        "import sqlite3\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "class TextToSQLRAGBot:\n",
        "    def __init__(\n",
        "        self,\n",
        "        sql_client,  # Client for the SQL generation model\n",
        "        response_client,  # Client for the response generation model\n",
        "        database_connection: sqlite3.Connection,\n",
        "        sql_model: str = \"microsoft/phi-4\",\n",
        "        response_model: str = \"meta-llama/Meta-Llama-3.1-70B-Instruct\",\n",
        "        get_sql_system_prompt = text_to_sql_system_prompt,\n",
        "        get_response_system_prompt = None\n",
        "    ):\n",
        "        \"\"\"Initialize the text-to-SQL RAG bot.\n",
        "\n",
        "        Args:\n",
        "            sql_client: Client for the SQL generation model (phi-4)\n",
        "            response_client: Client for the response generation model (Llama-70b)\n",
        "            database_connection: SQLite database connection\n",
        "            sql_model: The model to use for SQL generation\n",
        "            response_model: The model to use for response generation\n",
        "            get_sql_system_prompt: Function to retrieve the system message for SQL generation\n",
        "            get_response_system_prompt: Function to retrieve the system message for response generation\n",
        "        \"\"\"\n",
        "        self.sql_client = sql_client\n",
        "        self.response_client = response_client\n",
        "        self.sql_model = sql_model\n",
        "        self.response_model = response_model\n",
        "        self.db_conn = database_connection\n",
        "\n",
        "        self.get_sql_system_prompt = get_sql_system_prompt\n",
        "\n",
        "        # Default system message for response generation if none is provided\n",
        "        if get_response_system_prompt is None:\n",
        "            self.get_response_system_prompt = lambda: \"\"\"You are a helpful potion shop assistant that provides information based on database query results.\n",
        "\n",
        "You will be given:\n",
        "1. The original user question\n",
        "2. The SQL query that was generated to answer the question\n",
        "3. The results of executing that query on the potion shop database\n",
        "\n",
        "Formulate a natural, helpful response that answers the user's question based on the query results.\n",
        "Speak as if you are a knowledgeable potion shop employee helping a customer.\n",
        "If the query returned no results, explain that to the user in a friendly way.\n",
        "\"\"\"\n",
        "        else:\n",
        "            self.get_response_system_prompt = lambda: get_response_system_prompt\n",
        "\n",
        "    def generate_sql_query(self, user_question: str) -> str:\n",
        "        \"\"\"Generate an SQL query from a natural language question.\n",
        "\n",
        "        Args:\n",
        "            user_question: The natural language question from the user\n",
        "\n",
        "        Returns:\n",
        "            str: The generated SQL query\n",
        "        \"\"\"\n",
        "\n",
        "        try:\n",
        "            # Get SQL query from the model\n",
        "            completion = self.sql_client.chat.completions.create(\n",
        "                model=self.sql_model,\n",
        "                messages=[{\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": self.get_sql_system_prompt.format(question=user_question)\n",
        "                }]\n",
        "            )\n",
        "\n",
        "            response = completion.choices[0].message.content\n",
        "\n",
        "            # Look for SQL code blocks\n",
        "            sql_blocks = re.findall(r\"```sql\\s*(.*?)\\s*```\", response, re.DOTALL)\n",
        "            if sql_blocks:\n",
        "                sql_query = sql_blocks[-1].strip()\n",
        "            else:\n",
        "                # Look for generic code blocks\n",
        "                code_blocks = re.findall(r\"```\\s*(.*?)\\s*```\", response, re.DOTALL)\n",
        "                if code_blocks:\n",
        "                    sql_query = code_blocks[-1].strip()\n",
        "                else:\n",
        "                    # If no SQL-specific blocks, look for generic code blocks\n",
        "                    # Which will most likely result in a failure\n",
        "                    sql_query = response\n",
        "\n",
        "            return sql_query\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"Error generating SQL query: {str(e)}\"\n",
        "\n",
        "    def execute_query(self, sql_query: str) -> (pd.DataFrame, str):\n",
        "        \"\"\"Execute the SQL query on the database.\n",
        "\n",
        "        Args:\n",
        "            sql_query: The SQL query to execute\n",
        "\n",
        "        Returns:\n",
        "            tuple: (DataFrame with results, error message if any)\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Execute query and return results as a pandas DataFrame\n",
        "            results = pd.read_sql_query(sql_query, self.db_conn)\n",
        "            return results, None\n",
        "        except Exception as e:\n",
        "            return None, f\"Error executing SQL query: {str(e)}\"\n",
        "\n",
        "    def generate_response(self, user_question: str, sql_query: str, query_results: pd.DataFrame) -> str:\n",
        "        \"\"\"Generate a natural language response based on the query results.\n",
        "\n",
        "        Args:\n",
        "            user_question: The original user question\n",
        "            sql_query: The SQL query that was executed\n",
        "            query_results: The results of the query as a pandas DataFrame\n",
        "\n",
        "        Returns:\n",
        "            str: The natural language response\n",
        "        \"\"\"\n",
        "        messages = []\n",
        "        system_prompt = self.get_response_system_prompt()\n",
        "        if system_prompt:\n",
        "            messages.append({\n",
        "                \"role\": \"system\",\n",
        "                \"message\": system_prompt\n",
        "                })\n",
        "\n",
        "        # Convert DataFrame to string representation\n",
        "        if query_results is not None:\n",
        "            results_str = query_results.to_string()\n",
        "        else:\n",
        "            results_str = \"No results returned (query may have failed)\"\n",
        "\n",
        "        messages.append({\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"\"\"User question: {user_question}\n",
        "\n",
        "SQL query used:\n",
        "{sql_query}\n",
        "\n",
        "Query results:\n",
        "{results_str}\n",
        "\n",
        "Please provide a helpful response based on these results.\"\"\"\n",
        "        })\n",
        "\n",
        "        try:\n",
        "            # Get response from the model\n",
        "            completion = self.response_client.chat.completions.create(\n",
        "                model=self.response_model,\n",
        "                messages=messages\n",
        "            )\n",
        "\n",
        "            response = completion.choices[0].message.content\n",
        "            return response\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"Error generating response: {str(e)}\"\n",
        "\n",
        "    def chat(self, user_question: str) -> Dict[str, Any]:\n",
        "        \"\"\"Process a user question end-to-end.\n",
        "\n",
        "        Args:\n",
        "            user_question: The natural language question from the user\n",
        "\n",
        "        Returns:\n",
        "            dict: Dictionary containing the original question, generated SQL, query results, and final response\n",
        "        \"\"\"\n",
        "        # Step 1: Generate SQL query from user question\n",
        "        sql_query = self.generate_sql_query(user_question)\n",
        "\n",
        "        # Step 2: Execute the SQL query on the database\n",
        "        query_results, error = self.execute_query(sql_query)\n",
        "\n",
        "        # Step 3: Generate a natural language response based on the results\n",
        "        if error:\n",
        "            response = f\"I'm sorry, but I couldn't execute the query. {error}\"\n",
        "        else:\n",
        "            response = self.generate_response(user_question, sql_query, query_results)\n",
        "\n",
        "        # Return a dictionary with all components of the process\n",
        "        return {\n",
        "            \"user_question\": user_question,\n",
        "            \"generated_sql\": sql_query,\n",
        "            \"query_results\": query_results,\n",
        "            \"response\": response\n",
        "        }"
      ],
      "metadata": {
        "id": "W2rJtVlwN6ix"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's create an instance of the RAG bot and run several simple SQL queries:"
      ],
      "metadata": {
        "id": "qq7hudrWmXi8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "import sqlite3\n",
        "\n",
        "\n",
        "client = OpenAI(\n",
        "    base_url=\"https://api.studio.nebius.ai/v1/\",\n",
        "    api_key=os.environ.get(\"NEBIUS_API_KEY\"),\n",
        ")\n",
        "sql_model = \"microsoft/phi-4\"\n",
        "response_model = \"meta-llama/Meta-Llama-3.1-70B-Instruct\"\n",
        "\n",
        "sql_client = client\n",
        "response_client = client\n",
        "\n",
        "# Create or load the database\n",
        "db_conn = create_potion_shop_database()\n",
        "\n",
        "# Initialize the Text-to-SQL RAG bot\n",
        "text_to_sql_bot = TextToSQLRAGBot(\n",
        "    sql_client=client,\n",
        "    response_client=client,\n",
        "    database_connection=db_conn,\n",
        "    sql_model=sql_model,\n",
        "    response_model=response_model\n",
        ")\n",
        "\n",
        "# Example questions to test the bot\n",
        "example_questions = [\n",
        "    \"What are the most expensive potions?\",\n",
        "    \"Which customer has spent the most money at the shop?\",\n",
        "    \"How many healing potions do we have in stock?\",\n",
        "    \"What potions has Alaric purchased?\",\n",
        "    \"Which potions cause side effects related to dizziness or headaches?\"\n",
        "]\n",
        "\n",
        "# Process each question\n",
        "for i, question in enumerate(example_questions):\n",
        "    print(f\"\\n--- Example {i+1}: {question} ---\")\n",
        "\n",
        "    result = text_to_sql_bot.chat(question)\n",
        "\n",
        "    print(\"\\nGenerated SQL:\")\n",
        "    print(result[\"generated_sql\"])\n",
        "\n",
        "    print(\"\\nQuery Results:\")\n",
        "    if result[\"query_results\"] is not None:\n",
        "        print(result[\"query_results\"])\n",
        "    else:\n",
        "        print(\"No results (query may have failed)\")\n",
        "\n",
        "    print(\"\\nResponse:\")\n",
        "    print(result[\"response\"])\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HfPnQ4DULtY",
        "outputId": "b10e31e3-d078-4ca8-ff37-e15638798f59"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Database already exists at potion_shop.db\n",
            "\n",
            "--- Example 1: What are the most expensive potions? ---\n",
            "\n",
            "Generated SQL:\n",
            "SELECT p.potion_name, s.price\n",
            "FROM shop_inventory s\n",
            "JOIN potions p ON s.potion_id = p.potion_id\n",
            "ORDER BY s.price DESC;\n",
            "\n",
            "Query Results:\n",
            "                  potion_name  price\n",
            "0     Mighty Berserker Elixir     70\n",
            "1       Superior Speed Potion     65\n",
            "2        Major Healing Potion     60\n",
            "3     Dreamless Sleep Draught     50\n",
            "4           Major Mana Potion     45\n",
            "5     Refined Barkskin Potion     40\n",
            "6   Frenzied Berserker Elixir     35\n",
            "7          Swift Speed Potion     30\n",
            "8              Healing Potion     25\n",
            "9               Clarity Tonic     25\n",
            "10                Mana Potion     20\n",
            "11                   Antidote     20\n",
            "12      Crude Barkskin Potion     15\n",
            "13       Minor Healing Potion     10\n",
            "14          Minor Mana Potion      8\n",
            "\n",
            "Response:\n",
            "The top 5 most expensive potions are:\n",
            "\n",
            "1. Mighty Berserker Elixir ($70)\n",
            "2. Superior Speed Potion ($65)\n",
            "3. Major Healing Potion ($60)\n",
            "4. Dreamless Sleep Draught ($50)\n",
            "5. Major Mana Potion ($45)\n",
            "\n",
            "These potions are likely to have stronger effects or be more rare, which is reflected in their higher prices.\n",
            "\n",
            "============================================================\n",
            "\n",
            "--- Example 2: Which customer has spent the most money at the shop? ---\n",
            "\n",
            "Generated SQL:\n",
            "SELECT \n",
            "    p.customer_name,\n",
            "    SUM(s.price * p.quantity) AS total_spent\n",
            "FROM \n",
            "    purchases p\n",
            "JOIN \n",
            "    shop_inventory s ON p.potion_id = s.potion_id\n",
            "GROUP BY \n",
            "    p.customer_name\n",
            "ORDER BY \n",
            "    total_spent DESC\n",
            "LIMIT 1;\n",
            "\n",
            "Query Results:\n",
            "  customer_name  total_spent\n",
            "0        Thorne          260\n",
            "\n",
            "Response:\n",
            "The customer who has spent the most money at the shop is Thorne, with a total of $260.\n",
            "\n",
            "============================================================\n",
            "\n",
            "--- Example 3: How many healing potions do we have in stock? ---\n",
            "\n",
            "Generated SQL:\n",
            "SELECT SUM(si.stock) AS total_healing_potions_in_stock\n",
            "FROM shop_inventory si\n",
            "JOIN potions p ON si.potion_id = p.potion_id\n",
            "WHERE p.category = 'healing';\n",
            "\n",
            "Query Results:\n",
            "   total_healing_potions_in_stock\n",
            "0                              45\n",
            "\n",
            "Response:\n",
            "We currently have 45 healing potions in stock, with 15 of Intense Healing and 30 of Strong Healing.\n",
            "\n",
            "============================================================\n",
            "\n",
            "--- Example 4: What potions has Alaric purchased? ---\n",
            "\n",
            "Generated SQL:\n",
            "SELECT p.potion_name\n",
            "FROM purchases AS pr\n",
            "JOIN potions AS p ON pr.potion_id = p.potion_id\n",
            "WHERE pr.customer_name = 'Alaric';\n",
            "\n",
            "Query Results:\n",
            "            potion_name\n",
            "0     Minor Mana Potion\n",
            "1         Clarity Tonic\n",
            "2     Minor Mana Potion\n",
            "3  Minor Healing Potion\n",
            "\n",
            "Response:\n",
            "Alaric has purchased the following potions:\n",
            "\n",
            "1. Minor Mana Potion (twice)\n",
            "2. Clarity Tonic\n",
            "3. Minor Healing Potion\n",
            "\n",
            "============================================================\n",
            "\n",
            "--- Example 5: Which potions cause side effects related to dizziness or headaches? ---\n",
            "\n",
            "Generated SQL:\n",
            "SELECT potion_id, potion_name, side_effects\n",
            "FROM potions\n",
            "WHERE side_effects LIKE '%dizziness%' OR side_effects LIKE '%headaches%';\n",
            "\n",
            "Query Results:\n",
            "   potion_id         potion_name    side_effects\n",
            "0          9  Swift Speed Potion  mild dizziness\n",
            "\n",
            "Response:\n",
            "Based on the query results, one potion has been found that causes side effects related to dizziness or headaches:\n",
            "\n",
            "The Swift Speed Potion (ID: 9) has mild dizziness as a side effect.\n",
            "\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feel free to check if our bot answered correctly!"
      ],
      "metadata": {
        "id": "Ua4jdi9imk0F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, a less successful example:"
      ],
      "metadata": {
        "id": "kyXjYxvAcxqz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = text_to_sql_bot.chat(\"\"\"\n",
        "Today is 2024-05-01 (May 1st, 2024)\n",
        "How many of each of the following potions we had on 2024-02-01\n",
        "if we didn't have resupply since January:\n",
        "- Minor Healing Potion\n",
        "- Major Mana Potion\n",
        "- Swift Speed Potion?\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "NI-v6I6lXPV7"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(result[\"generated_sql\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bEvqJo-yYBk5",
        "outputId": "302a646c-e110-42ee-9278-b84b681e0c45"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SELECT \n",
            "    pot.potion_name,\n",
            "    (inv.stock - COALESCE(SUM(pur.quantity), 0)) AS remaining_stock\n",
            "FROM \n",
            "    potions pot\n",
            "JOIN \n",
            "    shop_inventory inv ON pot.potion_id = inv.potion_id\n",
            "LEFT JOIN \n",
            "    purchases pur ON pot.potion_id = pur.potion_id\n",
            "    AND pur.date < '2024-02-01'\n",
            "WHERE \n",
            "    pot.potion_name IN ('Minor Healing Potion', 'Major Mana Potion', 'Swift Speed Potion')\n",
            "GROUP BY \n",
            "    pot.potion_name, inv.stock;\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here the problem is that instead of adding all the purcases since 2024-02-01 to the current stock, the LLM suggests subtracting from today's stock the purchases that happened *before* 2024-02-01. This could be addressed with a more powerful LLM: Claude copes well with this request."
      ],
      "metadata": {
        "id": "aG5wXTvOc_37"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Uncomment this to close the database connection when done\n",
        "# db_conn.close()"
      ],
      "metadata": {
        "id": "Xej3K9jjYa72"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Beyond SQL\n",
        "\n",
        "Relational databases are great for working with structured data. However, sometimes you just need to store a pile of texts, or code, or JSONs, or, worse even, images or videos. Let's see how to deal with them!"
      ],
      "metadata": {
        "id": "zn_adYWhaJ3f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2: Vector Databases\n",
        "\n",
        "We can store whatever we want in a `(key, value)` format as long as the keys are convenient for retrieval. And it turns out that vectors are really good in this role!\n",
        "\n",
        "So, a **vector database** (or a **vector store**) stores data in key-value format with **vectors for keys**. Let's briefly discuss the overall pipeline of vector database usage."
      ],
      "metadata": {
        "id": "KBNN_w0TotZX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoders and embeddings\n",
        "\n",
        "To put an object into a vector store, you must **encode** it as a vector, or we might also say, you construct its **(vector) embedding**. This is done with a dedicated **encoder**, which is usually a neural network of sorts. You can learn more about **text encoders** in the [dedicated long read](https://nebius-academy.github.io/knowledge-base/text-encoders/). There are also a number of popular embedding models for images and videos (soon, we'll make a long read about them too).\n",
        "\n",
        "It's very important to point out that a good encoder doesn't just transform objects into vectors in some random way. Rather, the crucial feature for a good encoder is that **it maps semantically close objects to geometrically close vectors**. Thus, in a picture below, the vectors representing orcs cluster together, while the vectors representing cats are in a distance from them.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=14m2FfYjE_XUXGJkc4D8w4zzAlxVNVBrb\" width=600 />\n",
        "</center>\n",
        "\n",
        "As a measure of geometric proximity, either **Euclidean distance** is used or **cosine similarity**, while sometimes just the scalar product $(x, y) = x^Ty = x_1y_1+\\ldots+x_dy_d$ is used because of its computational simplicity.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1srHjEQuiGPoLaz9KEt8IXRxZol2mJjqG\" width=600 />\n",
        "</center>"
      ],
      "metadata": {
        "id": "huoc6vzQeVgG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrieval in vector stores\n",
        "\n",
        "Retrieval in a vector store relies on the **Nearest Neighbor Search** mechanism. A query to a vector database is an object like those stored in that database. What happens with the query is:\n",
        "\n",
        "1. First, the query is vectorized by the encoder to get its embedding.\n",
        "2. Then, the database finds the prescribed number of its nearest neighbor vectors.\n",
        "3. Finally, the database returns the objects corresponding to these vectors.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1ZeUSt-_VsBRZtHCfFNzde6iwiM9ltBgx\" width=600 />\n",
        "</center>\n",
        "\n",
        "**Note**. Models such as **CLIP** are able to encode both texts and images in a coherent way, mapping semantically related texts and images to geometrically close vectors. With such a model, you can, for example, have a database of images and query it with a text. You'll try this in the practice part."
      ],
      "metadata": {
        "id": "Hb7HiwCoc0xa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Of course, efficient retrieval requires quick nearest neighbour search, and for large databases or/and low-latency scenarios it's not a good idea to just cycle through all the items looking for the nearest one. So, there is a number of optimized strategies that pre-compute some additional data structures which allow for faster (even if approximate) neighbour search. One of the most popular ones is **HNSW** (**Hierarchical Navigable Small World**). If you're curious, we have a short explanation and a small demo further in this notebook."
      ],
      "metadata": {
        "id": "nO3TG-04oPvf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vector store zoo\n",
        "\n",
        "A huge variety of vector databases are available now, including [Chroma](https://docs.trychroma.com/docs/overview/getting-started), [LanceDB](https://lancedb.github.io/lancedb/), [Weaviate](https://weaviate.io/developers/weaviate), [Qdrant](https://qdrant.tech/documentation/) and many, many more - and choosing one is not a simple task. When you have less than several million vectors to store, it's actually more a question of a developer's convenience. While, at larger scales, the speed of pre-computation and nearest neighbor search may start contributing significantly to the overall efficiency.\n",
        "\n",
        "If you need to choose a vector store for your project, you can start by checking [this comparison chart](https://superlinked.com/vector-db-comparison), or a similar one."
      ],
      "metadata": {
        "id": "BAoi2_WIpB6A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RAG with LanceDB demo"
      ],
      "metadata": {
        "id": "zTX2TNoyo5ey"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this demo we'll apply RAG to perform Q&A about the `transformers` library using this library's documentation."
      ],
      "metadata": {
        "id": "Nkdqk-FsVMpd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q openai"
      ],
      "metadata": {
        "id": "ObJgMtLmIHZh"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "with open(\"nebius_api_key\", \"r\") as file:\n",
        "    nebius_api_key = file.read().strip()\n",
        "\n",
        "os.environ[\"NEBIUS_API_KEY\"] = nebius_api_key"
      ],
      "metadata": {
        "id": "rwwWg45B7cD7"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's get the docs."
      ],
      "metadata": {
        "id": "b_jSgZPQVYI9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text preprocessing\n",
        "\n",
        "Embedding magic won't work properly if the text is cluttered with formatting artifacts. Unless the formatting carries meaning (as it does in code), it's generally best to remove it before encoding.\n",
        "\n",
        "So, we're going to strip markdown from most of its structure leaving only plain text."
      ],
      "metadata": {
        "id": "kDRi6ApauG1P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "\n",
        "from tqdm import tqdm\n",
        "from bs4 import BeautifulSoup\n",
        "from markdown import markdown\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "def markdown_to_text(markdown_string):\n",
        "    \"\"\" Converts a markdown string to plaintext \"\"\"\n",
        "\n",
        "    # md -> html -> text since BeautifulSoup can extract text cleanly\n",
        "    html = markdown(markdown_string)\n",
        "\n",
        "    html = re.sub(r'<!--((.|\\n)*)-->', '', html)\n",
        "    html = re.sub('<code>bash', '<code>', html)\n",
        "\n",
        "    # extract text\n",
        "    soup = BeautifulSoup(html, \"html.parser\")\n",
        "    text = ''.join(soup.findAll(text=True))\n",
        "\n",
        "    text = re.sub('```(py|diff|python)', '', text)\n",
        "    text = re.sub('```\\n', '\\n', text)\n",
        "    text = re.sub('-         .*', '', text)\n",
        "    text = text.replace('...', '')\n",
        "    text = re.sub('\\n(\\n)+', '\\n\\n', text)\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "def prepare_files(input_dir=\"transformers/docs/source/en/\", output_dir=\"docs\"):\n",
        "    # Convert string paths to Path objects\n",
        "    input_dir = Path(input_dir)\n",
        "    output_dir = Path(output_dir)\n",
        "\n",
        "    # Check if input directory exists\n",
        "    assert input_dir.is_dir(), \"Input directory doesn't exist\"\n",
        "    output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    for root, subdirs, files in tqdm(os.walk(input_dir)):\n",
        "        root_path = Path(root)\n",
        "        for file_name in files:\n",
        "            file_path = root_path / file_name\n",
        "            parent = root_path.stem if root_path.stem != input_dir.stem else \"\"\n",
        "\n",
        "            if file_path.is_file():\n",
        "                with open(file_path, encoding=\"utf-8\") as f:\n",
        "                    md = f.read()\n",
        "                text = markdown_to_text(md)\n",
        "\n",
        "                output_file = output_dir / f\"{parent}_{Path(file_name).stem}.txt\"\n",
        "                with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "                    f.write(text)\n"
      ],
      "metadata": {
        "id": "V_lvjfBypJut"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/huggingface/transformers"
      ],
      "metadata": {
        "id": "pCYJQkEGos7x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a871a99-8c5f-4abf-d595-b061e2ea6904"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 278088, done.\u001b[K\n",
            "remote: Counting objects: 100% (133/133), done.\u001b[K\n",
            "remote: Compressing objects: 100% (95/95), done.\u001b[K\n",
            "remote: Total 278088 (delta 80), reused 40 (delta 36), pack-reused 277955 (from 3)\u001b[K\n",
            "Receiving objects: 100% (278088/278088), 289.12 MiB | 25.67 MiB/s, done.\n",
            "Resolving deltas: 100% (206671/206671), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prepare_files()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hAOn3XqBunGL",
        "outputId": "47989a4a-7308-46b6-f8ff-e0a7b27b4f0d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r0it [00:00, ?it/s]<ipython-input-8-d285d05c636f>:21: DeprecationWarning: Call to deprecated method findAll. (Replaced by find_all) -- Deprecated since version 4.0.0.\n",
            "  text = ''.join(soup.findAll(text=True))\n",
            "6it [00:07,  1.18s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chunking\n",
        "\n",
        "Text encoders may struggle with a long, information-rich text fragment. To make your encoder's life easier (and your RAG results better), you need to split your context into relatively small meaningful chunks.\n",
        "\n",
        "That said, there is no \"golden\" chunking strategy. (Ideally, each chunk should be a wholesome content piece dedicated to a single topic, but it rarely works this way.) By and large, one of the following two (simple) strategies are used:\n",
        "\n",
        "1. **Fixed token size with overlap**. This is a very straightforward approach, as it is likely to trip, crop, and split useful information. By overlapping chunks, we ensure that important information at the boundaries of each chunk is captured in multiple chunks, helping to mitigate the losses and maintain context continuity.\n",
        "\n",
        "Note that a chunk size is usually several hundreds of tokens. You can start with 512 and then tune it for your task if needed.\n",
        "\n",
        "1. **Recursive chunking**. First of all, this requires setting maximal chunk length in tokens. Now, as an example, you could split a plain text document like this:\n",
        "    - First, chunk by '\\n\\n' (that end of a \"section\").\n",
        "    - Then, if a \"section\" is too long, by '\\n' (by paragraph).\n",
        "    - Then by '.' (by sentence).\n",
        "    - At this point, if you're still above maximal length, just split by a fixed token length.\n",
        "\n",
        "Note that the separator sequence ['\\n\\n', '\\n', '.'] may be replaced by any other fit for your data. (Langchain's Recursive Character Text Splitter implements precisely this strategy.)\n",
        "\n",
        "Of course, you can use other specific features of your data to chunk it more meaningfully. For example, if you're dealing with markdown, it's wise to use specific separators for Recursive Chunking. To illustrate, in LangChain's Markdown Text Splitter, the following list is used:\n",
        "\n",
        "```\n",
        "[\n",
        "    \"\\\\n#{1,6} \",\n",
        "    \"```\\\\n\",\n",
        "    \"\\\\n\\\\*\\\\*\\\\*+\\\\n\",\n",
        "    \"\\\\n---+\\\\n\",\n",
        "    \"\\\\n___+\\\\n\",\n",
        "    \"\\\\n\\\\n\",\n",
        "    \"\\\\n\",\n",
        "    \" \",\n",
        "    \"\",\n",
        "]\n",
        "\n",
        "```\n",
        "\n",
        "The final \"\" indicates that we've just split by a fixed token length specified by the user at the setup of the Text Splitter tool if all separators are depleted.\n",
        "\n",
        "A good practice is to assess your chunks manually in order to check if their size is OK and that they stay meaningful under your chosen splitting strategy."
      ],
      "metadata": {
        "id": "fyhVtuugVa5B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will perform chunking using recursive text splitter from the `Langchain` library."
      ],
      "metadata": {
        "id": "n1EhOMX9VsIF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lancedb pyarrow tiktoken -q\n",
        "!pip install -qU langchain-text-splitters"
      ],
      "metadata": {
        "id": "sLFVdlajuzEV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4cdc7378-1f03-4750-ea78-fbe4221ba7df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.9/32.9 MB\u001b[0m \u001b[31m67.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    separators=[\n",
        "        \"\\n\\n\",\n",
        "        \"\\n\",\n",
        "        \".\",\n",
        "        \" \"\n",
        "    ],\n",
        "    chunk_size=1024,\n",
        "    chunk_overlap=128,\n",
        "    length_function=len,\n",
        "    is_separator_regex=False,\n",
        ")"
      ],
      "metadata": {
        "id": "oxYnDYSZ3qXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from typing import List\n",
        "from functools import partial\n",
        "\n",
        "import lancedb\n",
        "from lancedb.pydantic import LanceModel, Vector\n",
        "from lancedb.embeddings import get_registry"
      ],
      "metadata": {
        "id": "1QI3bcvB5CYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating a database\n",
        "\n",
        "Let's create a LanceDB database. For that, we'll need to define several things:\n",
        "\n",
        "1. The **embedding model**. It's set up with\n",
        "  \n",
        "  `embed_func = get_registry().get(<provider name>).create(name=<embedding name>)`\n",
        "  \n",
        "  For example,\n",
        "  \n",
        "  `embed_func = get_registry().get(\"openai\").create(name=\"text-embedding-ada-002\")`\n",
        "\n",
        "  We'll use a free model from Hugging Face for demonstration.\n",
        "\n",
        "2. The **database entry schema**. At the very least it should contain:\n",
        "  \n",
        "  * The **vector** field\n",
        "  * The **field which is vectorized**, in our case `text`. It's marked with `embed_func.SourceField()`"
      ],
      "metadata": {
        "id": "2wnXdy-aV1RM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This line is needed in case you've ran this cell before to clear the db dir\n",
        "!rm -rf /tmp/lancedb\n",
        "\n",
        "db = lancedb.connect(\"/tmp/lancedb\")"
      ],
      "metadata": {
        "id": "HdSR1he37Gjg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import pyarrow as pa\n",
        "\n",
        "# We use this model as the encoder: https://huggingface.co/BAAI/bge-small-en-v1.5\n",
        "embed_func = get_registry().get(\"huggingface\").create(name=\"BAAI/bge-small-en-v1.5\")\n",
        "\n",
        "\n",
        "class BasicSchema(LanceModel):\n",
        "    '''\n",
        "    This is how we store data in the database.\n",
        "    We need to have a vector here, but apart from this, we may have many other fields\n",
        "    '''\n",
        "    text: str = embed_func.SourceField()\n",
        "    vector: Vector(embed_func.ndims()) = embed_func.VectorField(default=None)\n",
        "\n",
        "lance_table = db.create_table(\n",
        "    \"transformer_docs\",\n",
        "    mode='overwrite',\n",
        "    schema=BasicSchema\n",
        ")"
      ],
      "metadata": {
        "id": "H_Dh1PYb7Mrf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333,
          "referenced_widgets": [
            "4a7708bd9a2943d5bf168c456fbe5f2d",
            "5a077a276f6b471c9cac1a2563611da9",
            "9e921fd800bd431c96d02ca6ce4266ba",
            "9f914bc6cf51474eb90cf15908888c0e",
            "9d3fdee74c1542e4a9eb9f108cf6af79",
            "dcd2c9631fa8468989d03f48b8748ea5",
            "a4b3df6f19a94b1f973b02c6020a7f0a",
            "5e442fdbc3bf42d0a910676193c12231",
            "06dc0c60a63d44588b2a0e1c6665ef86",
            "66e8f7d8153e460ebccd641f12146d23",
            "724f35d35c5c4dd499d164408b40ef3c",
            "007b5c70684d41a387c7b411768e9c7c",
            "389d697d01e04cec8ccb1c0e6cd909d3",
            "11714fd2d25e40858aa94458f03cd6bd",
            "aeaf396bf0a0403f9f14169ebf741581",
            "5ff75ee194ee41779e1a44a3205998ce",
            "81d8bf16525041c4950fea1d02abbc41",
            "30593430b4d6432d88c9b0b157e8f31a",
            "ee5b4ab3a6bc40309150cd014c177d4f",
            "6d2b223d419c43559e2a976e0d1beb28",
            "033ab85f0cad479bbc81f7a4372f0b44",
            "8cd8fa44d0fa414c82549fa5390b253b",
            "847b02f22cfd4f68a80e86b57605cd79",
            "cfa0409fca1343af9c19e648b2fd9d88",
            "864a4c9a4fd74de588d07fb2720b1951",
            "c04a940465074af1ada9b2d34c5c47cd",
            "72fd49b1e2d84d50ad893f236294efa7",
            "5cda57c7db11402d9a76bdd95c5e0237",
            "dba98e0a45af4ca5bdad1b3620e68af4",
            "a7b0eb13101c4fc6a3ad242e7bc0f44b",
            "bd5b35cf4c9e4e6dab2d89ee0baff241",
            "f2a1ef729f47485f968c03eff52a5f54",
            "8b40cea2169d4712b2623230365077c8",
            "5ec9fbb534b046969e5c499db898c9af",
            "a03f9e4aebf54262b35050d23fd4a6f7",
            "23d85a2625b44c408c10bce03ba0e105",
            "b4c98a6c3fe7449f990ebe18878736c2",
            "a6e4e82371974afc8f30a6eeb10de6de",
            "f08f2b1220504f0d8e1cd16606550999",
            "ad802f157e1e474ca310d38c68a87830",
            "9648d4ce66fa46c48d7e25ab7f8dfc8d",
            "91dc8d03e9b14e2f97ed33f103c3b03e",
            "976577f16ccd4d3b85eec41886c00e27",
            "698163451c9f48b3b6dc32c2a783ec82",
            "9bd86cd2513e44a48636aaa293e23529",
            "e7641138671b4d9fab8403d1c9be1b8f",
            "7a4bf04b98e34373959fe6e6551c57cf",
            "238095e4f2d041b0bf3418870194d746",
            "3317132b7a884da8b0c61440711cb3f1",
            "ea23cfdee41e49139d54114142d81887",
            "90a02eb383c04e52aba0fe8f3323d0f9",
            "d1193123531744ddb31f1c5d229bc22b",
            "f76344105cb74365accef684e14a55e8",
            "4f40053e76114c92a79cd20763ca60ea",
            "401476adc816447faa078552244c7625",
            "6a90d1fd038847e38af062a58ab1cff9",
            "37c8f9765da549f6bc7f14f3d661fbc7",
            "35c37f465cad4a20945fee0be6b87551",
            "0cc95c2735eb49da9941de8ae1bd0ece",
            "046d0e150cf34cfbb1785857e6bd3850",
            "643fba7fd4a54aacbfe94cea73b08253",
            "721504cb68b546f6bcbff3815b6b4c49",
            "e007c4368aef45518ea784413ee93593",
            "90ec33c0708941b29c672d63955f81b0",
            "55ee7a1c4d3b4d47937d812744a6fcd0",
            "8be6bf19662548b9974cf589abfdf214"
          ]
        },
        "outputId": "0a9169a9-4e29-45f2-cf3c-65699fe0d8a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4a7708bd9a2943d5bf168c456fbe5f2d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "007b5c70684d41a387c7b411768e9c7c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "847b02f22cfd4f68a80e86b57605cd79"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5ec9fbb534b046969e5c499db898c9af"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/743 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9bd86cd2513e44a48636aaa293e23529"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/133M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6a90d1fd038847e38af062a58ab1cff9"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's populate the database:"
      ],
      "metadata": {
        "id": "zdUN-O7hWAT0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "splitted_docs = []\n",
        "\n",
        "for file in tqdm(os.listdir(\"docs\")):\n",
        "    with open(\"docs/\"+file, \"r\") as f:\n",
        "        text = f.read()\n",
        "        docs = text_splitter.create_documents([text])\n",
        "        splitted_docs.extend([{\"text\": doc.page_content} for doc in docs])"
      ],
      "metadata": {
        "id": "7t_KCljz48s0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "084371f2-0e2a-4d63-8621-93945b4e8cc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 514/514 [00:00<00:00, 5246.10it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Total splits:\", len(splitted_docs))\n",
        "print(\"==First split:==\\n\", splitted_docs[0])\n",
        "print(\"==Second split:==\\n\", splitted_docs[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XY0bdvGy5LX7",
        "outputId": "ca2fc64f-dedd-4ce7-cffb-ec24da09c4e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total splits: 4423\n",
            "==First split:==\n",
            " {'text': 'Processors\\nMultimodal models require a preprocessor capable of handling inputs that combine more than one modality. Depending on the input modality, a processor needs to convert text into an array of tensors, images into pixel values, and audio into an array with tensors with the correct sampling rate.\\nFor example, PaliGemma is a vision-language model that uses the SigLIP image processor and the Llama tokenizer. A [ProcessorMixin] class wraps both of these preprocessor types, providing a single and unified processor class for a multimodal model.\\nCall [~ProcessorMixin.from_pretrained] to load a processor. Pass the input type to the processor to generate the expected model inputs, input ids and pixel values.'}\n",
            "==Second split:==\n",
            " {'text': 'from transformers import AutoProcessor, PaliGemmaForConditionalGeneration\\nfrom PIL import Image\\nimport requests\\nprocessor = AutoProcessor.from_pretrained(\"google/paligemma-3b-pt-224\")\\nprompt = \"answer en Where is the cat standing?\"\\nurl = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\"\\nimage = Image.open(requests.get(url, stream=True).raw)\\ninputs = processor(text=prompt, images=image, return_tensors=\"pt\")\\ninputs\\n\\nThis guide describes the processor class and how to preprocess multimodal inputs.\\nProcessor classes\\nAll processors inherit from the [ProcessorMixin] class which provides methods like [~ProcessorMixin.from_pretrained], [~ProcessorMixin.save_pretrained], and [~ProcessorMixin.push_to_hub] for loading, saving, and sharing processors to the Hub.\\nThere are two ways to load a processor, with an [AutoProcessor] and with a model-specific processor class.'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Populating the table may take some time:"
      ],
      "metadata": {
        "id": "JCrxbzBDJVUP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lance_table.add(\n",
        "    splitted_docs,\n",
        "    on_bad_vectors='drop'  # or 'fill' with fill_value=0.0\n",
        ")"
      ],
      "metadata": {
        "id": "ct4853ArFBEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll create a `search_table` function that will allow us to fetch up to `limits` documents from the database for our `query`:"
      ],
      "metadata": {
        "id": "_b9kQEJqWiOP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def search_table(table, query, max_results=5):\n",
        "    return table.search(query).limit(max_results).to_pydantic(BasicSchema)"
      ],
      "metadata": {
        "id": "NYzxAHKq-awe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = search_table(lance_table, \"How to load an LLM in 4 bit quantization?\", max_results=5)\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BwuEncKb-yf6",
        "outputId": "b7012e5d-d1f5-452d-aa7e-24e78e57fb5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[BasicSchema(text='Set up a [BitsAndBytesConfig] and set load_in_4bit=True to load a model in 4-bit precision. The [BitsAndBytesConfig] is passed to the quantization_config parameter in [~PreTrainedModel.from_pretrained].\\nAllow Accelerate to automatically distribute the model across your available hardware by setting device_map=“auto”.\\nPlace all inputs on the same device as the model.\\n\\nfrom transformers import BitsAndBytesConfig, AutoTokenizer, AutoModelForCausalLM\\nquantization_config = BitsAndBytesConfig(load_in_4bit=True)\\ntokenizer = AutoTokenizer(\"meta-llama/Llama-3.1-8B\")\\nmodel = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.1-8B\", device_map=\"auto\", quantization_config=quantization_config)\\nprompt = \"Hello, my llama is cute\"\\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(model_8bit.device)\\ngenerated_ids = model_8bit.generate(**inputs)\\noutputs = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)', vector=FixedSizeList(dim=384)),\n",
              " BasicSchema(text='Choose a quantization method suitable for your hardware and use case (see the Overview or Selecting a quantization method guide to help you).\\nLoad a pre-quantized model from the Hugging Face Hub or load a float32/float16/bfloat16 model and apply a specific quantization method with [QuantizationConfig].\\n\\nThe example below demonstrates loading a 8B parameter model and quantizing it to 4-bits with bitsandbytes.\\nthon\\nimport torch\\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\\nmodel_id = \"meta-llama/Llama-3.1-8B-Instruct\"\\nquantization_config = BitsAndBytesConfig(\\n    load_in_4bit=True,\\n    bnb_4bit_compute_dtype=torch.bfloat16\\n)\\nmodel = AutoModelForCausalLM.from_pretrained(\\n    model_id,\\n    quantization_config=quantization_config,\\n    torch_dtype=torch.bfloat16,\\n    device_map=\"auto\"\\n)\\n\\nResources\\nTo explore quantization and related performance optimization concepts more deeply, check out the following resources.', vector=FixedSizeList(dim=384)),\n",
              " BasicSchema(text='Load the quantized model by simply adding BitsAndBytesConfig as shown below:\\nthon\\nfrom transformers import VideoLlavaForConditionalGeneration, BitsAndBytesConfig\\nspecify how to quantize the model\\nquantization_config = BitsAndBytesConfig(\\n    load_in_4bit=True,\\n    bnb_4bit_quant_type=\"nf4\",\\n    bnb_4bit_compute_dtype=torch.float16,\\n)\\nmodel = VideoLlavaForConditionalGeneration.from_pretrained(\"LanguageBind/Video-LLaVA-7B-hf\", quantization_config=quantization_config, device_map=\"auto\")\\n\\nFlash-Attention 2 to speed-up generation\\nAdditionally, we can greatly speed-up model inference by using Flash Attention, which is a faster implementation of the attention mechanism used inside the model.\\nFirst, make sure to install the latest version of Flash Attention 2:', vector=FixedSizeList(dim=384)),\n",
              " BasicSchema(text='!pip install bitsandbytes\\nWe can then load models in 8-bit quantization by simply adding a load_in_8bit=True flag to from_pretrained.\\npython\\nmodel = AutoModelForCausalLM.from_pretrained(\"bigcode/octocoder\", load_in_8bit=True, pad_token_id=0)\\nNow, let\\'s run our example again and measure the memory usage.\\nthon\\npipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\\nresult = pipe(prompt, max_new_tokens=60)[0][\"generated_text\"][len(prompt):]\\nresult', vector=FixedSizeList(dim=384)),\n",
              " BasicSchema(text='To load a quantized model (8-bit or 4-bit), try bitsandbytes and set the load_in_4bit or load_in_8bit parameters to True. Loading the model in 8-bits only requires 6.87 GB of memory.\\n\\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\\nimport torch\\nquant_config = BitsAndBytesConfig(load_in_8bit=True)\\nmodel = AutoModelForCausalLM.from_pretrained(\\n    \"mistralai/Mistral-7B-v0.1\", quantization_config=quant_config, device_map=\"auto\"\\n)\\n```', vector=FixedSizeList(dim=384))]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = search_table(lance_table, \"LLM in 4 bit quantization\", max_results=5)\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jakrqjrX--rt",
        "outputId": "8e0d3e1b-5991-44df-d828-0a8d37c10e78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[BasicSchema(text='bitsandbytes\\nbitsandbytes features the LLM.int8 and QLoRA quantization to enable accessible large language model inference and training.\\nLLM.int8() is a quantization method that aims to make large language model inference more accessible without significant degradation. Unlike naive 8-bit quantization, which can result in loss of critical information and accuracy, LLM.int8() dynamically adapts to ensure sensitive components of the computation retain higher precision when needed.\\nQLoRA, or 4-bit quantization, compresses a model even further to 4-bits and inserts a small set of trainable low-rank adaptation (LoRA) weights to allowing training. \\n\\nNote: For a user-friendly quantization experience, you can use the bitsandbytes community space.\\n\\nRun the command below to install bitsandbytes.', vector=FixedSizeList(dim=384)),\n",
              " BasicSchema(text='Quantization Fundamentals with Hugging Face\\nQuantization in Depth\\nIntroduction to Quantization cooked in 🤗 with 💗🧑\\u200d🍳\\nEfficientML.ai Lecture 5 - Quantization Part I\\nMaking Deep Learning Go Brrrr From First Principles\\nAccelerating Generative AI with PyTorch Part 2: LLM Optimizations', vector=FixedSizeList(dim=384)),\n",
              " BasicSchema(text='Choose a quantization method suitable for your hardware and use case (see the Overview or Selecting a quantization method guide to help you).\\nLoad a pre-quantized model from the Hugging Face Hub or load a float32/float16/bfloat16 model and apply a specific quantization method with [QuantizationConfig].\\n\\nThe example below demonstrates loading a 8B parameter model and quantizing it to 4-bits with bitsandbytes.\\nthon\\nimport torch\\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\\nmodel_id = \"meta-llama/Llama-3.1-8B-Instruct\"\\nquantization_config = BitsAndBytesConfig(\\n    load_in_4bit=True,\\n    bnb_4bit_compute_dtype=torch.bfloat16\\n)\\nmodel = AutoModelForCausalLM.from_pretrained(\\n    model_id,\\n    quantization_config=quantization_config,\\n    torch_dtype=torch.bfloat16,\\n    device_map=\"auto\"\\n)\\n\\nResources\\nTo explore quantization and related performance optimization concepts more deeply, check out the following resources.', vector=FixedSizeList(dim=384)),\n",
              " BasicSchema(text=\"Resources\\nLearn more about the details of 8-bit quantization in A Gentle Introduction to 8-bit Matrix Multiplication for transformers at scale using Hugging Face Transformers, Accelerate and bitsandbytes.\\nTry 4-bit quantization in this notebook and learn more about it's details in Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA.\", vector=FixedSizeList(dim=384)),\n",
              " BasicSchema(text=\"If GPU memory is not a constraint for your use case, there is often no need to look into quantization. However many GPUs simply can't run LLMs without quantization methods and in this case, 4-bit and 8-bit quantization schemes are extremely useful tools.\\nFor more in-detail usage information, we strongly recommend taking a look at the Transformers Quantization Docs.\\nNext, let's look into how we can improve computational and memory efficiency by using better algorithms and an improved model architecture.\\n2. Flash Attention\\nToday's top-performing LLMs share more or less the same fundamental architecture that consists of feed-forward layers, activation layers, layer normalization layers, and most crucially, self-attention layers.\\nSelf-attention layers are central to Large Language Models (LLMs) in that they enable the model to understand the contextual relationships between input tokens.\", vector=FixedSizeList(dim=384))]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you see, the search results may vary as you reformulate the query."
      ],
      "metadata": {
        "id": "-kfPWoRfJ3TV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Answering questions with RAG\n",
        "\n",
        "Finally, it's time to generate something!\n",
        "\n",
        "In the previous notebook the `answer_with_db` function used web search; we'll update it to use database search instead."
      ],
      "metadata": {
        "id": "7VPnNKC2WssQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "import os\n",
        "\n",
        "nebius_client = OpenAI(\n",
        "    base_url=\"https://api.studio.nebius.ai/v1/\",\n",
        "    api_key=os.environ.get(\"NEBIUS_API_KEY\"),\n",
        ")\n",
        "llama_8b_model = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
        "\n",
        "def prettify_string(text, max_line_length=80):\n",
        "    \"\"\"Prints a string with line breaks at spaces to prevent horizontal scrolling.\n",
        "    Args:\n",
        "        text: The string to print.\n",
        "        max_line_length: The maximum length of each line.\n",
        "    \"\"\"\n",
        "    output_lines = []\n",
        "    lines = text.split(\"\\n\")\n",
        "    for line in lines:\n",
        "        current_line = \"\"\n",
        "        words = line.split()\n",
        "        for word in words:\n",
        "            if len(current_line) + len(word) + 1 <= max_line_length:\n",
        "                current_line += word + \" \"\n",
        "            else:\n",
        "                output_lines.append(current_line.strip())\n",
        "                current_line = word + \" \"\n",
        "        output_lines.append(current_line.strip())  # Append the last line\n",
        "    return \"\\n\".join(output_lines)\n",
        "\n",
        "def search_result_to_context(search_result):\n",
        "    return \"\\n\\n\".join(\n",
        "        [record.text for record in search_result]\n",
        "    )\n",
        "\n",
        "def answer_with_rag(\n",
        "    prompt: str,\n",
        "    system_prompt=None,\n",
        "    max_tokens=512,\n",
        "    client=nebius_client,\n",
        "    model=llama_8b_model,\n",
        "    table=None,\n",
        "    prettify=True,\n",
        "    temperature=0.6,\n",
        "    max_results=5,\n",
        "    verbose=False\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Generate an answer using RAG (Retrieval-Augmented Generation) with database search.\n",
        "\n",
        "    Args:\n",
        "        prompt: User's question or prompt\n",
        "        system_prompt: Instructions for the LLM\n",
        "        max_tokens: Maximum number of tokens in the response\n",
        "        client: OpenAI client instance\n",
        "        model: Model identifier\n",
        "        table: LanceDB table\n",
        "        prettify: Whether to format the output text\n",
        "        temperature: Temperature for response generation\n",
        "        max_results: Maximal number of documents to fetch from the table\n",
        "        verbose: whether to return the search results as well\n",
        "\n",
        "    Returns:\n",
        "        Generated response incorporating search results\n",
        "    \"\"\"\n",
        "    # Perform database search\n",
        "    if table:\n",
        "        try:\n",
        "            search_results = search_table(table, prompt, max_results=max_results)\n",
        "        except:\n",
        "            search_results = []\n",
        "    else:\n",
        "        search_results = []\n",
        "\n",
        "    # Construct messages with search results\n",
        "    messages = []\n",
        "\n",
        "    if system_prompt:\n",
        "        messages.append({\n",
        "            \"role\": \"system\",\n",
        "            \"content\": system_prompt\n",
        "        })\n",
        "\n",
        "    # Add user prompt\n",
        "    messages.append({\n",
        "        \"role\": \"user\",\n",
        "        \"content\":\n",
        "            f\"\"\"Answer the following query using the context provided.\n",
        "\n",
        "            <context>\\n{search_results}\\n</context>\n",
        "\n",
        "            <query>{prompt}</query>\n",
        "            \"\"\"\n",
        "    })\n",
        "\n",
        "    # Generate completion\n",
        "    completion = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        max_tokens=max_tokens,\n",
        "        temperature=temperature\n",
        "    )\n",
        "\n",
        "    if prettify:\n",
        "        answer = prettify_string(completion.choices[0].message.content)\n",
        "    else:\n",
        "        answer = completion.choices[0].message.content\n",
        "\n",
        "    if verbose:\n",
        "        return {\n",
        "            \"answer\": answer,\n",
        "            \"search_results\": search_results\n",
        "        }\n",
        "    else:\n",
        "        return answer"
      ],
      "metadata": {
        "id": "hiTEmVES7oL3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's first try asking Llama how to load an LLM in 4 bit quantization without supplying context (`table=None`)."
      ],
      "metadata": {
        "id": "va-Cfq11KmDo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "client = OpenAI(\n",
        "    base_url=\"https://api.studio.nebius.ai/v1/\",\n",
        "    api_key=os.environ.get(\"NEBIUS_API_KEY\"),\n",
        ")\n",
        "model = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
        "\n",
        "results = answer_with_rag(\"\"\"How to load an LLM in 4 bit quantization?\"\"\",\n",
        "               client=client, model=model, table=None, verbose=True)\n",
        "print(results[\"answer\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ax1IEAepKz2W",
        "outputId": "3218ecdc-3d7b-4f43-8ea1-e886b524d981"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unfortunately, there is no context provided to give a specific answer to the\n",
            "query. However, I can provide a general outline on how to load a Large Language\n",
            "Model (LLM) in 4-bit quantization.\n",
            "\n",
            "**4-bit Quantization**\n",
            "\n",
            "4-bit quantization is a technique used to reduce the precision of neural\n",
            "network weights and activations from 32-bit floating-point numbers to 4-bit\n",
            "integers. This reduces the memory footprint and computational requirements of\n",
            "the model.\n",
            "\n",
            "**Loading an LLM in 4-bit Quantization**\n",
            "\n",
            "To load an LLM in 4-bit quantization, you'll need to follow these general\n",
            "steps:\n",
            "\n",
            "1. **Quantize the LLM weights**: Use a quantization library or framework (e.g.,\n",
            "TensorFlow, PyTorch, or ONNX) to convert the LLM weights from 32-bit\n",
            "floating-point numbers to 4-bit integers. This involves mapping the weight\n",
            "values to a smaller range of values that can be represented by 4-bit integers.\n",
            "2. **Choose a quantization strategy**: Select a quantization strategy, such as:\n",
            "* **Weight-wise quantization**: Quantize each weight independently.\n",
            "* **Activation-wise quantization**: Quantize the activations of the model.\n",
            "* **Mixed-precision quantization**: Combine weight-wise and activation-wise\n",
            "quantization.\n",
            "3. **Load the quantized LLM weights**: Load the quantized weights into the LLM\n",
            "model using the chosen framework or library.\n",
            "4. **Configure the LLM for 4-bit quantization**: Update the LLM's configuration\n",
            "to use 4-bit integers for computations and weight updates.\n",
            "5. **Test the quantized LLM**: Evaluate the performance of the quantized LLM on\n",
            "a test dataset to ensure that it meets the desired accuracy and performance\n",
            "requirements.\n",
            "\n",
            "**Example Frameworks and Libraries**\n",
            "\n",
            "Some popular frameworks and libraries that support 4-bit quantization include:\n",
            "\n",
            "* TensorFlow: `tf.quantization`\n",
            "* PyTorch: `torch.quantization`\n",
            "* ONNX: `onnxruntime`\n",
            "* Intel's OpenVINO: `openvino`\n",
            "\n",
            "**Note**: The specific steps and configuration may vary depending on the chosen\n",
            "framework, library, or LLM architecture. It's essential to consult the\n",
            "documentation and examples provided by the framework or library to ensure\n",
            "successful quantization and deployment of the LLM.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you see, the answer is vague and not qualified. Now, let's add proper context!"
      ],
      "metadata": {
        "id": "zGh_gpZ9KvuE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "client = OpenAI(\n",
        "    base_url=\"https://api.studio.nebius.ai/v1/\",\n",
        "    api_key=os.environ.get(\"NEBIUS_API_KEY\"),\n",
        ")\n",
        "model = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
        "\n",
        "results = answer_with_rag(\"\"\"How to load an LLM in 4 bit quantization?\"\"\",\n",
        "               client=client, model=model, table=lance_table, verbose=True)\n",
        "print(results[\"answer\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PXuaSbrwDKkD",
        "outputId": "62d0c7f8-9c05-47e2-f3e0-000ed94ef841"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To load an LLM in 4-bit quantization, you can use the `BitsAndBytesConfig`\n",
            "class from the `transformers` library. Here is an example of how you can do it:\n",
            "\n",
            "```python\n",
            "from transformers import AutoModelForCausalLM, AutoTokenizer,\n",
            "BitsAndBytesConfig\n",
            "\n",
            "# Load the model and tokenizer\n",
            "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B\")\n",
            "model = AutoModelForCausalLM.from_pretrained(\n",
            "\"meta-llama/Llama-3.1-8B\",\n",
            "quantization_config=BitsAndBytesConfig(load_in_4bit=True),\n",
            "device_map=\"auto\"\n",
            ")\n",
            "```\n",
            "\n",
            "In this example, the `BitsAndBytesConfig` class is used to specify the\n",
            "quantization configuration. The `load_in_4bit=True` parameter tells the model\n",
            "to load in 4-bit precision. The `device_map=\"auto\"` parameter allows the model\n",
            "to automatically distribute itself across the available hardware.\n",
            "\n",
            "Note that you need to make sure that the model you are loading is compatible\n",
            "with 4-bit quantization. Also, the performance benefits of 4-bit quantization\n",
            "may vary depending on the specific hardware and use case.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "That's much better!"
      ],
      "metadata": {
        "id": "7UQHE5hwK9lA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results[\"search_results\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MkXNQe-OOYI_",
        "outputId": "d41a640f-5bda-4b79-9891-8c32ca4f2c41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[BasicSchema(text='Set up a [BitsAndBytesConfig] and set load_in_4bit=True to load a model in 4-bit precision. The [BitsAndBytesConfig] is passed to the quantization_config parameter in [~PreTrainedModel.from_pretrained].\\nAllow Accelerate to automatically distribute the model across your available hardware by setting device_map=“auto”.\\nPlace all inputs on the same device as the model.\\n\\nfrom transformers import BitsAndBytesConfig, AutoTokenizer, AutoModelForCausalLM\\nquantization_config = BitsAndBytesConfig(load_in_4bit=True)\\ntokenizer = AutoTokenizer(\"meta-llama/Llama-3.1-8B\")\\nmodel = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.1-8B\", device_map=\"auto\", quantization_config=quantization_config)\\nprompt = \"Hello, my llama is cute\"\\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(model_8bit.device)\\ngenerated_ids = model_8bit.generate(**inputs)\\noutputs = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)', vector=FixedSizeList(dim=384)),\n",
              " BasicSchema(text='Choose a quantization method suitable for your hardware and use case (see the Overview or Selecting a quantization method guide to help you).\\nLoad a pre-quantized model from the Hugging Face Hub or load a float32/float16/bfloat16 model and apply a specific quantization method with [QuantizationConfig].\\n\\nThe example below demonstrates loading a 8B parameter model and quantizing it to 4-bits with bitsandbytes.\\nthon\\nimport torch\\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\\nmodel_id = \"meta-llama/Llama-3.1-8B-Instruct\"\\nquantization_config = BitsAndBytesConfig(\\n    load_in_4bit=True,\\n    bnb_4bit_compute_dtype=torch.bfloat16\\n)\\nmodel = AutoModelForCausalLM.from_pretrained(\\n    model_id,\\n    quantization_config=quantization_config,\\n    torch_dtype=torch.bfloat16,\\n    device_map=\"auto\"\\n)\\n\\nResources\\nTo explore quantization and related performance optimization concepts more deeply, check out the following resources.', vector=FixedSizeList(dim=384)),\n",
              " BasicSchema(text='Load the quantized model by simply adding BitsAndBytesConfig as shown below:\\nthon\\nfrom transformers import VideoLlavaForConditionalGeneration, BitsAndBytesConfig\\nspecify how to quantize the model\\nquantization_config = BitsAndBytesConfig(\\n    load_in_4bit=True,\\n    bnb_4bit_quant_type=\"nf4\",\\n    bnb_4bit_compute_dtype=torch.float16,\\n)\\nmodel = VideoLlavaForConditionalGeneration.from_pretrained(\"LanguageBind/Video-LLaVA-7B-hf\", quantization_config=quantization_config, device_map=\"auto\")\\n\\nFlash-Attention 2 to speed-up generation\\nAdditionally, we can greatly speed-up model inference by using Flash Attention, which is a faster implementation of the attention mechanism used inside the model.\\nFirst, make sure to install the latest version of Flash Attention 2:', vector=FixedSizeList(dim=384)),\n",
              " BasicSchema(text='!pip install bitsandbytes\\nWe can then load models in 8-bit quantization by simply adding a load_in_8bit=True flag to from_pretrained.\\npython\\nmodel = AutoModelForCausalLM.from_pretrained(\"bigcode/octocoder\", load_in_8bit=True, pad_token_id=0)\\nNow, let\\'s run our example again and measure the memory usage.\\nthon\\npipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\\nresult = pipe(prompt, max_new_tokens=60)[0][\"generated_text\"][len(prompt):]\\nresult', vector=FixedSizeList(dim=384)),\n",
              " BasicSchema(text='To load a quantized model (8-bit or 4-bit), try bitsandbytes and set the load_in_4bit or load_in_8bit parameters to True. Loading the model in 8-bits only requires 6.87 GB of memory.\\n\\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\\nimport torch\\nquant_config = BitsAndBytesConfig(load_in_8bit=True)\\nmodel = AutoModelForCausalLM.from_pretrained(\\n    \"mistralai/Mistral-7B-v0.1\", quantization_config=quant_config, device_map=\"auto\"\\n)\\n```', vector=FixedSizeList(dim=384))]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zDfjNXmnQIzx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Practice part\n",
        "\n",
        "If you encounter any difficulties or simply want to see our solutions, feel free to check the [Solutions notebook](https://colab.research.google.com/github/Nebius-Academy/LLM-Engineering-Essentials/blob/main/topic3/3.2_database_search_and_vector_stores_solutuons.ipynb)."
      ],
      "metadata": {
        "id": "AUuVJ9wnQJYr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1. A simple recommender system\n",
        "\n",
        "Vector search is a reasonable baseline for either retrieval or recommendations. In this task, we'll test it on a simple database containing information about goods that might be sold by a fantasy trader. (Of course, those could be any items an online store would sell.)\n",
        "\n",
        "Let's download the data."
      ],
      "metadata": {
        "id": "veKS2e3AiOlj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/Nebius-Academy/LLM-Engineering-Essentials/raw/main/topic3/fantasy_items_descriptions.json -O fantasy_items_descriptions.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jLWDfOOMigP6",
        "outputId": "385274e5-45ce-446e-88ca-c89a33378604"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-19 01:35:38--  https://github.com/Nebius-Academy/LLM-Engineering-Essentials/raw/main/topic3/fantasy_items_descriptions.json\n",
            "Resolving github.com (github.com)... 140.82.112.3\n",
            "Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/Nebius-Academy/LLM-Engineering-Essentials/main/topic3/fantasy_items_descriptions.json [following]\n",
            "--2025-04-19 01:35:38--  https://raw.githubusercontent.com/Nebius-Academy/LLM-Engineering-Essentials/main/topic3/fantasy_items_descriptions.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 361418 (353K) [text/plain]\n",
            "Saving to: ‘fantasy_items_descriptions.json’\n",
            "\n",
            "fantasy_items_descr 100%[===================>] 352.95K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2025-04-19 01:35:38 (7.68 MB/s) - ‘fantasy_items_descriptions.json’ saved [361418/361418]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open('fantasy_items_descriptions.json', 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "data[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nHEyVV3LsDc6",
        "outputId": "07a39294-1101-4f24-cdb4-753d111552aa"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'name': 'Enhanced Minor Healing Potion',\n",
              " 'category': 'Potion',\n",
              " 'rarity': 'Uncommon',\n",
              " 'structured': {'name': 'Enhanced Minor Healing Potion',\n",
              "  'category': 'Potion',\n",
              "  'rarity': 'Uncommon',\n",
              "  'appearance': 'A delicate glass vial filled with a rich, golden liquid that shimmers softly in the light and has a faint scent of honey and rose petals.',\n",
              "  'effect': 'Restores a moderate amount of health (typically 2d8 + 2 HP or equivalent) and grants a slight boost to vitality, allowing the drinker to ignore the first point of damage they take in the next hour.',\n",
              "  'usage_instructions': 'Drink the entire contents of the vial slowly, feeling the soothing warmth spread through the body.',\n",
              "  'duration': 'Instant healing, with the vitality boost lasting for 1 hour.',\n",
              "  'side_effects': 'A gentle tingling sensation in the fingers and toes, and a subtle feeling of rejuvenation.',\n",
              "  'origin_lore': 'Brewed by skilled herbalists who infuse the mixture with prayers to the gods of healing and protection, these potions are sought after by adventurers and guardsmen alike.',\n",
              "  'ingredients_components': 'Bloodleaf extract, powdered sunroot, holy water, essence of rose petals, and a drizzle of honey harvested under the full moon.',\n",
              "  'price': '50 gold pieces',\n",
              "  'sellers_pitch': \"A step up from the usual fare, this potion won't just keep you standing – it'll give you the edge you need to turn the tide of battle in your favor!\"},\n",
              " 'vague_description': '**Healer\\'s Gift**\\nThey say, in the oldest of days, skilled hands would weave prayers and petals into liquid gold. This potion\\'s one such tale come true. Inside a vial so delicate, it looks almost otherworldly, is a drink that\\'ll warm your bones and lift the weight from your shoulders. Sip it slow, let the sweetness spread, and by the time you\\'re done, you\\'ll be standing a little taller. Pain won\\'t find you as easily, not for a little while. They claim it\\'s the essence of rose petals and honey—harvested when the moon is just right—that lends this brew its gentle power.\\n\\nIn whispers, it\\'s said that herbalists would whisper to the mixture as they worked, hoping the gods of healing would lend an ear. Whether truth or tale, the result is a drink that mends what\\'s broken, and for an hour or so, you might even feel invincible.\\n\\n*\"It won\\'t mend your past, but it\\'ll hold you together long enough to face your future.\"*'}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you see, we have quite a lot of information about every item. It is presented it two ways:\n",
        "\n",
        "* structured info in the `structured` field\n",
        "* text description which retells the same information in a playful and somewhat distorted wat in the `vague_description` field.\n",
        "\n",
        "Now, your task will be to create a two-stage recommender system, that for a user's query will:\n",
        "\n",
        "* First, perform a database search to find 5-10 candidates,\n",
        "* Then, query an LLM to analyze the search results and choose 2-3 final candidates. This way, the LLM will play the role of a **reranker**. We'll talk more about rerankers in the next notebook.\n",
        "\n",
        "We also encourage you to do the following experiments:\n",
        "\n",
        "* Try and compare retrieval that uses either of the three databases:\n",
        "  \n",
        "  * A database consisting of only vague descriptions\n",
        "  * A database containing structured descriptions in JSON format\n",
        "  * A database containing structured descriptions in pure text format, with all JSON markup stripped\n",
        "\n",
        "* Try and compare retrieval with and without query preprocessing. In such scenarios it's often beneficial to reformulate a query, making it less colloquial and more to the point\n",
        "\n",
        "We'll discuss automatic RAG scoring in one of the following weeks. Right now, we suggest just gathering 10 diverse queries and looking closely at the outputs."
      ],
      "metadata": {
        "id": "FzG8rNdPsO0J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# <YOUR CODE HERE>"
      ],
      "metadata": {
        "id": "wN8ctEm39Hfv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2. Trying another vector database - an exercise in LLM-assisted development\n",
        "\n",
        "There are many vector stores, and you might need to use different ones depending on the tastes of your colleagues and various other considerations. In this task, you'll try [Qdrant](https://qdrant.tech/), which is also a convenient and efficient database.\n",
        "\n",
        "We suggest that you use an in-memory client for this exercise.\n",
        "\n",
        "And since we don't provide any explanations, the best way of coping with this task will be to ask an LLM to translate the above code from LanceDB to Qdrant! Our advice is using an LLM that can also perform web search (ChatGPT or Gemini, for example), because the libraries may change quite rapidly. This shouldn't be problematic, but if you struggle with the task, feel free to check our solution. Be prepared though that the code might fail once or twice before you fix it with the help of the LLM."
      ],
      "metadata": {
        "id": "1BxN8DRl_Qvx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll start with the usual preparations."
      ],
      "metadata": {
        "id": "EHBcFIl5MF1R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "\n",
        "from tqdm import tqdm\n",
        "from bs4 import BeautifulSoup\n",
        "from markdown import markdown\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "def markdown_to_text(markdown_string):\n",
        "    \"\"\" Converts a markdown string to plaintext \"\"\"\n",
        "\n",
        "    # md -> html -> text since BeautifulSoup can extract text cleanly\n",
        "    html = markdown(markdown_string)\n",
        "\n",
        "    html = re.sub(r'<!--((.|\\n)*)-->', '', html)\n",
        "    html = re.sub('<code>bash', '<code>', html)\n",
        "\n",
        "    # extract text\n",
        "    soup = BeautifulSoup(html, \"html.parser\")\n",
        "    text = ''.join(soup.findAll(text=True))\n",
        "\n",
        "    text = re.sub('```(py|diff|python)', '', text)\n",
        "    text = re.sub('```\\n', '\\n', text)\n",
        "    text = re.sub('-         .*', '', text)\n",
        "    text = text.replace('...', '')\n",
        "    text = re.sub('\\n(\\n)+', '\\n\\n', text)\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "def prepare_files(input_dir=\"transformers/docs/source/en/\", output_dir=\"docs\"):\n",
        "    # Convert string paths to Path objects\n",
        "    input_dir = Path(input_dir)\n",
        "    output_dir = Path(output_dir)\n",
        "\n",
        "    # Check if input directory exists\n",
        "    assert input_dir.is_dir(), \"Input directory doesn't exist\"\n",
        "    output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    for root, subdirs, files in tqdm(os.walk(input_dir)):\n",
        "        root_path = Path(root)\n",
        "        for file_name in files:\n",
        "            file_path = root_path / file_name\n",
        "            parent = root_path.stem if root_path.stem != input_dir.stem else \"\"\n",
        "\n",
        "            if file_path.is_file():\n",
        "                with open(file_path, encoding=\"utf-8\") as f:\n",
        "                    md = f.read()\n",
        "                text = markdown_to_text(md)\n",
        "\n",
        "                output_file = output_dir / f\"{parent}_{Path(file_name).stem}.txt\"\n",
        "                with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "                    f.write(text)\n"
      ],
      "metadata": {
        "id": "IE1oAw3CE4sD"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And now it's your turn to ingest the data into a Qdrant database instance and to update the `answer_with_rag` function accordingly!"
      ],
      "metadata": {
        "id": "HtB43TgkMNud"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# <YOUR CODE HERE>"
      ],
      "metadata": {
        "id": "rwe0e32r_maw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 3. A safer text2sql device\n",
        "\n",
        "When you combine SQL and LLMs, you might face the challenge of determining the amount of freedom you're ready to provide to the agent. Can it only execute `SELECT` queries? Or do you allow it to modify your tables in certain ways? The more freedom you give it, the more you risk losing data or getting it changed in unpredictable ways.\n",
        "\n",
        "Our `text_to_sql_bot` uses\n",
        "\n",
        "```\n",
        "pd.read_sql_query(sql_query, self.db_conn)\n",
        "```\n",
        "\n",
        "to run queries. This function is designed to execute queries that return rows (which are mostly `SELECT` queries) and it will fail to execute a query that manipulates with tables or data (so, no `DROP`, `DELETE`, `CREATE` etc).\n",
        "\n",
        "But what if our SQL bot actually needed to alter the table? For example, to edit side effects based on customers' feedback?\n",
        "\n",
        "In this task, you'll give it a try — and you might get a little bruised along the way. What we suggest you to do:\n",
        "\n",
        "**Step 1.** Try to update the bot, introducing as few ad-hoc changes as possible. Namely:\n",
        "\n",
        "- Change `pd.read_sql_query(sql_query, self.db_conn)` into `query_db(self.db_conn, sql_query)` and relax the system prompt guidelines a bit to allow for some changes - namely for adding side effects to potions.\n",
        "\n",
        "**Step 2.** No matter what you write in the system prompt, you've actually invited havoc into your life when you allowed execution of random queries. Test it by making the bot delete data about Thorne's purchases. Thorne is our top-spending customer; you can use it ;)\n",
        "\n",
        "Recall the jailbreaking techniques from Topic 1. Execise all you guile and ingenuity! It won't be that complicated in the end.\n",
        "\n",
        "**Step 3.** Now, try to make the bot safe again. The basic principle here is: you can't forbid an LLM from doing anything wiht just prompting. So, if you want a really safe system, you'll need to do some hardcoding. Here are some possible solutions you could explore:\n",
        "\n",
        "* Introducing simple checks that the `sql_query` generated by the LLM doesn't contain bad words such as `DELETE`, `DROP` etc.\n",
        "* Adding on top of that a layer of LLM defense: for non-SELECT queries it would decide whether it's benign or malicious. Again, no LLM is 100% failure-proof, but tricking an LLM whose input and output are concealed from you might be way more complicated.\n",
        "* Instead of performing checks after generating the SQL query, you can make it in the very beginning. Just add an LLM classifier to discern between two situations:\n",
        "\n",
        "  - A potential `SELECT` statement for which the safe `pd.read_sql_query(sql_query, self.db_conn)` will be called\n",
        "  - A request for adding a specific side effect to a specific potion - these could be obtained with JSON outputs. Then, you'll just plug them into a ready-made SQL query template!\n",
        "\n",
        "This all doesn't seem too much exciting - it's almost as if we don't believe in the magic of LLMs... But the thing is: magic is cool but only as long as your buiseness processes can't get hurt by its side effects.\n",
        "\n",
        "So be safe and take the power of LLMs with a grain of salt!"
      ],
      "metadata": {
        "id": "uh1MFDWwe05V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# <YOUR CODE HERE>"
      ],
      "metadata": {
        "id": "qajsWma5uPdH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
