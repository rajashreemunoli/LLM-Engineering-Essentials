{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nebius-Academy/LLM-Engineering-Essentials/blob/main/topic3/3.3_advanced_rag_components_solutions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM Engineering Essentials 3.3. Advanced RAG components"
      ],
      "metadata": {
        "id": "QIgO73GPq22_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Practice solutions"
      ],
      "metadata": {
        "id": "ztjX8BwcVD-V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1. Adding a reranker\n",
        "\n",
        "In this task, you'll need to add the **reranking stage** to the `answer_with_rag` function.\n",
        "\n",
        "Compare the results with and without reranking and with different reranking models. Try to come up with tricky and confusing prompts."
      ],
      "metadata": {
        "id": "3YX7so4cX-SP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Solution**. Here is our implementation. The interesting part starts with `answer_with_rag`. So just scroll till that moment!"
      ],
      "metadata": {
        "id": "2fZnO1GyYOzH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lancedb pyarrow tiktoken -q\n",
        "!pip install -qU langchain-text-splitters"
      ],
      "metadata": {
        "id": "2-eylR7ediPf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    separators=[\n",
        "        \"\\n\\n\",\n",
        "        \"\\n\",\n",
        "        \".\",\n",
        "        \" \"\n",
        "    ],\n",
        "    chunk_size=1024,\n",
        "    chunk_overlap=128,\n",
        "    length_function=len,\n",
        "    is_separator_regex=False,\n",
        ")"
      ],
      "metadata": {
        "id": "HSQtLO_cdiPf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from typing import List\n",
        "from functools import partial\n",
        "\n",
        "import lancedb\n",
        "from lancedb.pydantic import LanceModel, Vector\n",
        "from lancedb.embeddings import get_registry\n",
        "\n",
        "import openai\n",
        "import pyarrow as pa"
      ],
      "metadata": {
        "id": "Ux-p70dSdiPg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "\n",
        "from tqdm import tqdm\n",
        "from bs4 import BeautifulSoup\n",
        "from markdown import markdown\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "def markdown_to_text(markdown_string):\n",
        "    \"\"\" Converts a markdown string to plaintext \"\"\"\n",
        "\n",
        "    # md -> html -> text since BeautifulSoup can extract text cleanly\n",
        "    html = markdown(markdown_string)\n",
        "\n",
        "    html = re.sub(r'<!--((.|\\n)*)-->', '', html)\n",
        "    html = re.sub('<code>bash', '<code>', html)\n",
        "\n",
        "    # extract text\n",
        "    soup = BeautifulSoup(html, \"html.parser\")\n",
        "    text = ''.join(soup.findAll(text=True))\n",
        "\n",
        "    text = re.sub('```(py|diff|python)', '', text)\n",
        "    text = re.sub('```\\n', '\\n', text)\n",
        "    text = re.sub('-         .*', '', text)\n",
        "    text = text.replace('...', '')\n",
        "    text = re.sub('\\n(\\n)+', '\\n\\n', text)\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "def prepare_files(input_dir=\"transformers/docs/source/en/\", output_dir=\"docs\"):\n",
        "    # Convert string paths to Path objects\n",
        "    input_dir = Path(input_dir)\n",
        "    output_dir = Path(output_dir)\n",
        "\n",
        "    # Check if input directory exists\n",
        "    assert input_dir.is_dir(), \"Input directory doesn't exist\"\n",
        "    output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    for root, subdirs, files in tqdm(os.walk(input_dir)):\n",
        "        root_path = Path(root)\n",
        "        for file_name in files:\n",
        "            file_path = root_path / file_name\n",
        "            parent = root_path.stem if root_path.stem != input_dir.stem else \"\"\n",
        "\n",
        "            if file_path.is_file():\n",
        "                with open(file_path, encoding=\"utf-8\") as f:\n",
        "                    md = f.read()\n",
        "                text = markdown_to_text(md)\n",
        "\n",
        "                output_file = output_dir / f\"{parent}_{Path(file_name).stem}.txt\"\n",
        "                with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "                    f.write(text)\n"
      ],
      "metadata": {
        "id": "_i5MUrTsew8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/huggingface/transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hiVA36IEew8H",
        "outputId": "78eb173b-7e35-42df-be12-3fd5833e6daf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 278088, done.\u001b[K\n",
            "remote: Counting objects: 100% (133/133), done.\u001b[K\n",
            "remote: Compressing objects: 100% (95/95), done.\u001b[K\n",
            "remote: Total 278088 (delta 80), reused 40 (delta 36), pack-reused 277955 (from 3)\u001b[K\n",
            "Receiving objects: 100% (278088/278088), 289.32 MiB | 23.16 MiB/s, done.\n",
            "Resolving deltas: 100% (206618/206618), done.\n",
            "Updating files: 100% (4934/4934), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prepare_files()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "606c59bf-3a09-448a-baf4-2b8920542eb4",
        "id": "-U8qSrqtew8I"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r0it [00:00, ?it/s]<ipython-input-242-d285d05c636f>:21: DeprecationWarning: Call to deprecated method findAll. (Replaced by find_all) -- Deprecated since version 4.0.0.\n",
            "  text = ''.join(soup.findAll(text=True))\n",
            "6it [00:06,  1.13s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This line is needed in case you've ran this cell before to clear the db dir\n",
        "!rm -rf /tmp/lancedb\n",
        "\n",
        "db = lancedb.connect(\"/tmp/lancedb\")\n",
        "\n",
        "# We use this model as the encoder: https://huggingface.co/BAAI/bge-small-en-v1.5\n",
        "embed_func = get_registry().get(\"huggingface\").create(name=\"BAAI/bge-small-en-v1.5\")\n",
        "\n",
        "\n",
        "class BasicSchema(LanceModel):\n",
        "    '''\n",
        "    This is how we store data in the database.\n",
        "    We need to have a vector here, but apart from this, we may have many other fields\n",
        "    '''\n",
        "    text: str = embed_func.SourceField()\n",
        "    vector: Vector(embed_func.ndims()) = embed_func.VectorField(default=None)\n",
        "\n",
        "lance_table = db.create_table(\n",
        "    \"transformer_docs\",\n",
        "    mode='overwrite',\n",
        "    schema=BasicSchema\n",
        ")\n",
        "\n",
        "# Populating the database\n",
        "\n",
        "from tqdm import tqdm\n",
        "splitted_docs = []\n",
        "\n",
        "for file in tqdm(os.listdir(\"docs\")):\n",
        "    with open(\"docs/\"+file, \"r\") as f:\n",
        "        text = f.read()\n",
        "        docs = text_splitter.create_documents([text])\n",
        "        splitted_docs.extend([{\"text\": doc.page_content} for doc in docs])\n",
        "\n",
        "lance_table.add(\n",
        "    splitted_docs,\n",
        "    on_bad_vectors='drop'  # or 'fill' with fill_value=0.0\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02GseFXJdnBR",
        "outputId": "8fd6e327-ae44-42cf-8a47-620c25229e75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 515/515 [00:00<00:00, 4816.30it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---\n",
        "\n",
        "Now, the interesting part.\n",
        "\n",
        "There's not much difference, to tell the truth. It can be summarized in the following snippet:\n",
        "\n",
        "```python\n",
        "    # Perform database search\n",
        "    if table:\n",
        "        try:\n",
        "            stage_1_results = search_table(table, prompt,\n",
        "                                           max_results=max_stage_1_results)\n",
        "\n",
        "            # Here comes the reranker!\n",
        "            if reranker_model:\n",
        "                stage_1_docs = search_results_to_text(stage_1_results)\n",
        "                search_results = reranker_model.rank(\n",
        "                    prompt, stage_1_docs, return_documents=True, top_k=max_results\n",
        "                )\n",
        "            else:\n",
        "                search_results = search_result_to_context(stage_1_results)\n",
        "```"
      ],
      "metadata": {
        "id": "vh61n5GOAJP3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from math import e\n",
        "from openai import OpenAI\n",
        "import os\n",
        "from sentence_transformers import CrossEncoder\n",
        "\n",
        "nebius_client = OpenAI(\n",
        "    base_url=\"https://api.studio.nebius.ai/v1/\",\n",
        "    api_key=os.environ.get(\"NEBIUS_API_KEY\"),\n",
        ")\n",
        "llama_8b_model = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
        "\n",
        "reranker_model = CrossEncoder(\"mixedbread-ai/mxbai-rerank-base-v1\")\n",
        "\n",
        "def prettify_string(text, max_line_length=80):\n",
        "    \"\"\"Prints a string with line breaks at spaces to prevent horizontal scrolling.\n",
        "    Args:\n",
        "        text: The string to print.\n",
        "        max_line_length: The maximum length of each line.\n",
        "    \"\"\"\n",
        "    output_lines = []\n",
        "    lines = text.split(\"\\n\")\n",
        "    for line in lines:\n",
        "        current_line = \"\"\n",
        "        words = line.split()\n",
        "        for word in words:\n",
        "            if len(current_line) + len(word) + 1 <= max_line_length:\n",
        "                current_line += word + \" \"\n",
        "            else:\n",
        "                output_lines.append(current_line.strip())\n",
        "                current_line = word + \" \"\n",
        "        output_lines.append(current_line.strip())  # Append the last line\n",
        "    return \"\\n\".join(output_lines)\n",
        "\n",
        "def search_table(table, query, max_results=15):\n",
        "    return table.search(query).limit(max_results).to_pydantic(BasicSchema)\n",
        "\n",
        "def search_result_to_context(search_result):\n",
        "    return \"\\n\\n\".join(\n",
        "        [record.text for record in search_result]\n",
        "    )\n",
        "\n",
        "def search_results_to_text(search_result):\n",
        "    return [record.text for record in search_result]\n",
        "\n",
        "def answer_with_rag(\n",
        "    prompt: str,\n",
        "    system_prompt=None,\n",
        "    max_tokens=512,\n",
        "    client=nebius_client,\n",
        "    model=llama_8b_model,\n",
        "    reranker_model=reranker_model,\n",
        "    table=None,\n",
        "    prettify=True,\n",
        "    temperature=0.6,\n",
        "    max_stage_1_results=15,\n",
        "    max_results=5,\n",
        "    verbose=False\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Generate an answer using RAG (Retrieval-Augmented Generation) with database search.\n",
        "\n",
        "    Args:\n",
        "        prompt: User's question or prompt\n",
        "        system_prompt: Instructions for the LLM\n",
        "        max_tokens: Maximum number of tokens in the response\n",
        "        client: OpenAI client instance\n",
        "        model: Model identifier\n",
        "        search_client: Search client instance (for example, Tavily)\n",
        "        prettify: Whether to format the output text\n",
        "        temperature: Temperature for response generation\n",
        "        search_depth: Depth of web search ('basic' or 'advanced')\n",
        "        verbose: whether to return the search results as well\n",
        "\n",
        "    Returns:\n",
        "        Generated response incorporating search results\n",
        "    \"\"\"\n",
        "    # Perform database search\n",
        "    if table:\n",
        "        try:\n",
        "            stage_1_results = search_table(table, prompt,\n",
        "                                           max_results=max_stage_1_results)\n",
        "\n",
        "            # Here comes the reranker!\n",
        "            if reranker_model:\n",
        "                stage_1_docs = search_results_to_text(stage_1_results)\n",
        "                search_results = reranker_model.rank(\n",
        "                    prompt, stage_1_docs, return_documents=True, top_k=max_results\n",
        "                )\n",
        "            else:\n",
        "                search_results = search_result_to_context(stage_1_results)\n",
        "\n",
        "        except (AttributeError, ValueError) as err:\n",
        "            print(err)\n",
        "            stage_1_results = []\n",
        "            search_results = []\n",
        "    else:\n",
        "        stage_1_results = []\n",
        "        search_results = []\n",
        "\n",
        "    # Construct messages with search results\n",
        "    messages = []\n",
        "\n",
        "    if system_prompt:\n",
        "        messages.append({\n",
        "            \"role\": \"system\",\n",
        "            \"content\": system_prompt\n",
        "        })\n",
        "\n",
        "    # Add user prompt\n",
        "    messages.append({\n",
        "        \"role\": \"user\",\n",
        "        \"content\":\n",
        "            f\"\"\"Answer the following query using the context provided.\n",
        "\n",
        "            <context>\\n{search_results}\\n</context>\n",
        "\n",
        "            <query>{prompt}</query>\n",
        "            \"\"\"\n",
        "    })\n",
        "\n",
        "    # Generate completion\n",
        "    completion = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        max_tokens=max_tokens,\n",
        "        temperature=temperature\n",
        "    )\n",
        "\n",
        "    if prettify:\n",
        "        answer = prettify_string(completion.choices[0].message.content)\n",
        "    else:\n",
        "        answer = completion.choices[0].message.content\n",
        "\n",
        "    if verbose:\n",
        "        return {\n",
        "            \"answer\": answer,\n",
        "            \"stage_1_results\": stage_1_results,\n",
        "            \"search_results\": search_results\n",
        "        }\n",
        "    else:\n",
        "        return answer"
      ],
      "metadata": {
        "id": "xrUTpK--g3CB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = OpenAI(\n",
        "    base_url=\"https://api.studio.nebius.ai/v1/\",\n",
        "    api_key=os.environ.get(\"NEBIUS_API_KEY\"),\n",
        ")\n",
        "model = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
        "\n",
        "reranker_model = CrossEncoder(\"mixedbread-ai/mxbai-rerank-base-v1\")\n",
        "\n",
        "results = answer_with_rag(\"\"\"How to quantize a model in 4 bits?\"\"\",\n",
        "               client=client, model=model, reranker_model=reranker_model,\n",
        "               table=lance_table, verbose=True,\n",
        "               max_stage_1_results=15, max_results=5)\n",
        "print(results[\"answer\"])"
      ],
      "metadata": {
        "id": "99sH-wUAg3GL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51d3a37c-11d4-4c9a-899a-41f4e220e9ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Based on the provided context, the query can be answered as follows:\n",
            "\n",
            "To quantize a model in 4 bits, you can use the `BitsAndBytesConfig` class from\n",
            "the `transformers` library. Here is an example of how to do it:\n",
            "\n",
            "```python\n",
            "import torch\n",
            "from transformers import AutoModelForCausalLM, AutoTokenizer,\n",
            "BitsAndBytesConfig\n",
            "\n",
            "model_id = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
            "quantization_config = BitsAndBytesConfig(\n",
            "load_in_4bit=True,\n",
            "bnb_4bit_compute_dtype=torch.bfloat16\n",
            ")\n",
            "\n",
            "model = AutoModelForCausalLM.from_pretrained(\n",
            "model_id,\n",
            "quantization_config=quantization_config,\n",
            "torch_dtype=torch.bfloat16,\n",
            "device_map=\"auto\"\n",
            ")\n",
            "```\n",
            "\n",
            "This code creates a `BitsAndBytesConfig` instance with `load_in_4bit=True` and\n",
            "`bnb_4bit_compute_dtype=torch.bfloat16`, which tells the library to load the\n",
            "model in 4 bits and use bfloat16 as the compute dtype. Then, it uses the\n",
            "`from_pretrained` method to load the model with the specified quantization\n",
            "configuration.\n",
            "\n",
            "Alternatively, you can also use the example provided in the context, which is:\n",
            "\n",
            "```python\n",
            "import torch\n",
            "from transformers import AutoTokenizer, AutoModelForCausalLM,\n",
            "BitsAndBytesConfig\n",
            "\n",
            "quantization_config = BitsAndBytesConfig(\n",
            "load_in_4bit=True,\n",
            "bnb_4bit_compute_dtype=torch.bfloat16,\n",
            "bnb_4bit_quant_type=\"nf4\",\n",
            "bnb_4bit_use_double_quant=True,\n",
            ")\n",
            "\n",
            "tokenizer = AutoTokenizer.from_pretrained(\"tiiuae/falcon-7b\")\n",
            "model = AutoModelForCausalLM.from_pretrained(\n",
            "\"tiiuae/falcon-7b\",\n",
            "torch_dtype=torch.bfloat16,\n",
            "device_map=\"auto\",\n",
            "quantization_config=quantization_config,\n",
            ")\n",
            "```\n",
            "\n",
            "This code creates a `BitsAndBytesConfig` instance with additional parameters,\n",
            "and then uses the `from_pretrained` method to load the model with the specified\n",
            "quantization configuration.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look at the retrieved context pieces and their reranker scores:"
      ],
      "metadata": {
        "id": "OuinAg_FCOXe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results"
      ],
      "metadata": {
        "id": "ZYMFn5Lrg3In",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ab2ce8b-8e0c-4825-a8db-f152d2ec7da5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'answer': 'Based on the provided context, the query can be answered as follows:\\n\\nTo quantize a model in 4 bits, you can use the `BitsAndBytesConfig` class from\\nthe `transformers` library. Here is an example of how to do it:\\n\\n```python\\nimport torch\\nfrom transformers import AutoModelForCausalLM, AutoTokenizer,\\nBitsAndBytesConfig\\n\\nmodel_id = \"meta-llama/Llama-3.1-8B-Instruct\"\\nquantization_config = BitsAndBytesConfig(\\nload_in_4bit=True,\\nbnb_4bit_compute_dtype=torch.bfloat16\\n)\\n\\nmodel = AutoModelForCausalLM.from_pretrained(\\nmodel_id,\\nquantization_config=quantization_config,\\ntorch_dtype=torch.bfloat16,\\ndevice_map=\"auto\"\\n)\\n```\\n\\nThis code creates a `BitsAndBytesConfig` instance with `load_in_4bit=True` and\\n`bnb_4bit_compute_dtype=torch.bfloat16`, which tells the library to load the\\nmodel in 4 bits and use bfloat16 as the compute dtype. Then, it uses the\\n`from_pretrained` method to load the model with the specified quantization\\nconfiguration.\\n\\nAlternatively, you can also use the example provided in the context, which is:\\n\\n```python\\nimport torch\\nfrom transformers import AutoTokenizer, AutoModelForCausalLM,\\nBitsAndBytesConfig\\n\\nquantization_config = BitsAndBytesConfig(\\nload_in_4bit=True,\\nbnb_4bit_compute_dtype=torch.bfloat16,\\nbnb_4bit_quant_type=\"nf4\",\\nbnb_4bit_use_double_quant=True,\\n)\\n\\ntokenizer = AutoTokenizer.from_pretrained(\"tiiuae/falcon-7b\")\\nmodel = AutoModelForCausalLM.from_pretrained(\\n\"tiiuae/falcon-7b\",\\ntorch_dtype=torch.bfloat16,\\ndevice_map=\"auto\",\\nquantization_config=quantization_config,\\n)\\n```\\n\\nThis code creates a `BitsAndBytesConfig` instance with additional parameters,\\nand then uses the `from_pretrained` method to load the model with the specified\\nquantization configuration.',\n",
              " 'stage_1_results': [BasicSchema(text=\"Quantizing a model is as simple as passing a quantization_config to the model. One can change the code snippet above with the changes below. We'll leverage the BitsAndyBytes quantization (but refer to this page for other quantization methods):\", vector=FixedSizeList(dim=384)),\n",
              "  BasicSchema(text='[!WARNING]\\n8 and 4-bit training is only supported for training extra parameters.\\n\\nCheck your memory footprint with get_memory_footprint.\\npy\\nprint(model.get_memory_footprint())\\nLoad quantized models with [~PreTrainedModel.from_pretrained] without a quantization_config.\\n\\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\\nmodel = AutoModelForCausalLM.from_pretrained(\"{your_username}/bloom-560m-8bit\", device_map=\"auto\")\\n\\nLLM.int8\\nThis section explores some of the specific features of 8-bit quantization, such as offloading, outlier thresholds, skipping module conversion, and finetuning.\\nOffloading\\n8-bit models can offload weights between the CPU and GPU to fit very large models into memory. The weights dispatched to the CPU are stored in float32 and aren\\'t converted to 8-bit. For example, enable offloading for bigscience/bloom-1b7 through [BitsAndBytesConfig].', vector=FixedSizeList(dim=384)),\n",
              "  BasicSchema(text='!pip install bitsandbytes\\nWe can then load models in 8-bit quantization by simply adding a load_in_8bit=True flag to from_pretrained.\\npython\\nmodel = AutoModelForCausalLM.from_pretrained(\"bigcode/octocoder\", load_in_8bit=True, pad_token_id=0)\\nNow, let\\'s run our example again and measure the memory usage.\\nthon\\npipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\\nresult = pipe(prompt, max_new_tokens=60)[0][\"generated_text\"][len(prompt):]\\nresult', vector=FixedSizeList(dim=384)),\n",
              "  BasicSchema(text='Model optimization\\nQuantization using bitsandbytes\\nThe model can be loaded in 8 or 4 bits, greatly reducing the memory requirements while maintaining the performance of the original model. First make sure to install bitsandbytes, pip install bitsandbytes and make sure to have access to a GPU/accelerator that is supported by the library.\\n\\nbitsandbytes is being refactored to support multiple backends beyond CUDA. Currently, ROCm (AMD GPU) and Intel CPU implementations are mature, with Intel XPU in progress and Apple Silicon support expected by Q4/Q1. For installation instructions and the latest backend updates, visit this link.\\nWe value your feedback to help identify bugs before the full release! Check out these docs for more details and feedback links.', vector=FixedSizeList(dim=384)),\n",
              "  BasicSchema(text='Learn how to quantize models in the Quantization guide.\\n\\nQuantoConfig\\n[[autodoc]] QuantoConfig\\nAqlmConfig\\n[[autodoc]] AqlmConfig\\nVptqConfig\\n[[autodoc]] VptqConfig\\nAwqConfig\\n[[autodoc]] AwqConfig\\nEetqConfig\\n[[autodoc]] EetqConfig\\nGPTQConfig\\n[[autodoc]] GPTQConfig\\nBitsAndBytesConfig\\n[[autodoc]] BitsAndBytesConfig\\nHfQuantizer\\n[[autodoc]] quantizers.base.HfQuantizer\\nHiggsConfig\\n[[autodoc]] HiggsConfig\\nHqqConfig\\n[[autodoc]] HqqConfig\\nFbgemmFp8Config\\n[[autodoc]] FbgemmFp8Config\\nCompressedTensorsConfig\\n[[autodoc]] CompressedTensorsConfig\\nTorchAoConfig\\n[[autodoc]] TorchAoConfig\\nBitNetConfig\\n[[autodoc]] BitNetConfig\\nSpQRConfig\\n[[autodoc]] SpQRConfig\\nFineGrainedFP8Config\\n[[autodoc]] FineGrainedFP8Config\\nQuarkConfig\\n[[autodoc]] QuarkConfig', vector=FixedSizeList(dim=384)),\n",
              "  BasicSchema(text='Model optimization\\nQuantization using Bitsandbytes\\nThe model can be loaded in 8 or 4 bits, greatly reducing the memory requirements while maintaining the performance of the original model. First make sure to install bitsandbytes, pip install bitsandbytes, and to have access to a GPU/accelerator that is supported by the library.\\n\\nbitsandbytes is being refactored to support multiple backends beyond CUDA. Currently, ROCm (AMD GPU) and Intel CPU implementations are mature, with Intel XPU in progress and Apple Silicon support expected by Q4/Q1. For installation instructions and the latest backend updates, visit this link.\\nWe value your feedback to help identify bugs before the full release! Check out these docs for more details and feedback links.', vector=FixedSizeList(dim=384)),\n",
              "  BasicSchema(text='Model optimization\\nQuantization using Bitsandbytes\\nThe model can be loaded in 8 or 4 bits, greatly reducing the memory requirements while maintaining the performance of the original model. First make sure to install bitsandbytes, pip install bitsandbytes and to have access to a GPU/accelerator that is supported by the library.\\n\\nbitsandbytes is being refactored to support multiple backends beyond CUDA. Currently, ROCm (AMD GPU) and Intel CPU implementations are mature, with Intel XPU in progress and Apple Silicon support expected by Q4/Q1. For installation instructions and the latest backend updates, visit this link.\\nWe value your feedback to help identify bugs before the full release! Check out these docs for more details and feedback links.', vector=FixedSizeList(dim=384)),\n",
              "  BasicSchema(text='bitsandbytes\\nbitsandbytes features the LLM.int8 and QLoRA quantization to enable accessible large language model inference and training.\\nLLM.int8() is a quantization method that aims to make large language model inference more accessible without significant degradation. Unlike naive 8-bit quantization, which can result in loss of critical information and accuracy, LLM.int8() dynamically adapts to ensure sensitive components of the computation retain higher precision when needed.\\nQLoRA, or 4-bit quantization, compresses a model even further to 4-bits and inserts a small set of trainable low-rank adaptation (LoRA) weights to allowing training. \\n\\nNote: For a user-friendly quantization experience, you can use the bitsandbytes community space.\\n\\nRun the command below to install bitsandbytes.', vector=FixedSizeList(dim=384)),\n",
              "  BasicSchema(text='Choose a quantization method suitable for your hardware and use case (see the Overview or Selecting a quantization method guide to help you).\\nLoad a pre-quantized model from the Hugging Face Hub or load a float32/float16/bfloat16 model and apply a specific quantization method with [QuantizationConfig].\\n\\nThe example below demonstrates loading a 8B parameter model and quantizing it to 4-bits with bitsandbytes.\\nthon\\nimport torch\\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\\nmodel_id = \"meta-llama/Llama-3.1-8B-Instruct\"\\nquantization_config = BitsAndBytesConfig(\\n    load_in_4bit=True,\\n    bnb_4bit_compute_dtype=torch.bfloat16\\n)\\nmodel = AutoModelForCausalLM.from_pretrained(\\n    model_id,\\n    quantization_config=quantization_config,\\n    torch_dtype=torch.bfloat16,\\n    device_map=\"auto\"\\n)\\n\\nResources\\nTo explore quantization and related performance optimization concepts more deeply, check out the following resources.', vector=FixedSizeList(dim=384)),\n",
              "  BasicSchema(text='echo -e \"# Function to calculate the factorial of a number\\\\ndef factorial(n):\" | transformers-cli run --task text-generation --model meta-llama/CodeLlama-7b-hf --device 0\\n\\nQuantization reduces the memory burden of large models by representing the weights in a lower precision. Refer to the Quantization overview for more available quantization backends.\\nThe example below uses bitsandbytes to only quantize the weights to 4-bits.', vector=FixedSizeList(dim=384)),\n",
              "  BasicSchema(text='Batched multi-image input and quantization with BitsAndBytes\\nThis implementation of the Mistral3 models supports batched text-images inputs with different number of images for each text.\\nThis example also how to use BitsAndBytes to load the model in 4bit quantization.\\nthon', vector=FixedSizeList(dim=384)),\n",
              "  BasicSchema(text='Quantize a model by creating a [HqqConfig] and specifying the nbits and group_size to replace for all the linear layers (torch.nn.Linear) of the model.\\n``` py\\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, HqqConfig\\nquant_config = HqqConfig(nbits=8, group_size=64)\\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\\n    \"meta-llama/Llama-3.1-8B\", \\n    torch_dtype=torch.float16, \\n    device_map=\"cuda\", \\n    quantization_config=quant_config\\n)', vector=FixedSizeList(dim=384)),\n",
              "  BasicSchema(text=\"Output:\\nHere is a Python function that transforms bytes to Giga bytes:\\\\n\\\\n\\\\ndef bytes_to_gigabytes(bytes):\\\\n    return bytes / 1024 / 1024 / 1024\\\\n\\\\n\\\\nThis function takes a single argument\\nWe're almost seeing the same output text as before - just the python is missing just before the code snippet. Let's see how much memory was required.\\npython\\nbytes_to_giga_bytes(torch.cuda.max_memory_allocated())\\nOutput:\\n9.543574333190918\\nJust 9.5GB! That's really not a lot for a >15 billion parameter model.\\nWhile we see very little degradation in accuracy for our model here, 4-bit quantization can in practice often lead to different results compared to 8-bit quantization or full bfloat16 inference. It is up to the user to try it out.\\nAlso note that inference here was again a bit slower compared to 8-bit quantization which is due to the more aggressive quantization method used for 4-bit quantization leading to \\\\( \\\\text{quantize} \\\\) and \\\\( \\\\text{dequantize} \\\\) taking longer during inference.\\npython\\ndel model\\ndel pipe\\npython\", vector=FixedSizeList(dim=384)),\n",
              "  BasicSchema(text='The model name is Meta-Llama-3.1-70B-Instruct.\\nThe number of centroids is given by 65536 (2^16).\\nThe number of residual centroids is given by 256 (2^8).\\n\\nThe equivalent bit-width calculation is given by the following.\\n\\nindex: log2(65536) = 16 / 8 = 2-bits\\nresidual index: log2(256) = 8 / 8 = 1-bit\\ntotal bit-width: 2 + 1 = 3-bits\\n\\nFrom here, estimate the model size by multiplying 70B * 3-bits / 8-bits/byte for a total of 26.25GB.\\nLoad a VPTQ quantized model with [~PreTrainedModel.from_pretrained].\\n\\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\\nquantized_model = AutoModelForCausalLM.from_pretrained(\\n    \"VPTQ-community/Meta-Llama-3.1-70B-Instruct-v16-k65536-65536-woft\",\\n    torch_dtype=\"auto\", \\n    device_map=\"auto\"\\n)', vector=FixedSizeList(dim=384)),\n",
              "  BasicSchema(text='Quantization reduces the memory burden of large models by representing the weights in a lower precision. Refer to the Quantization overview for more available quantization backends.\\nThe example below uses bitsandbytes to only quantize the weights to 4-bits.\\nthon\\nimport torch\\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\\nquantization_config = BitsAndBytesConfig(\\n    load_in_4bit=True,\\n    bnb_4bit_compute_dtype=torch.bfloat16,\\n    bnb_4bit_quant_type=\"nf4\",\\n    bnb_4bit_use_double_quant=True,\\n)\\ntokenizer = AutoTokenizer.from_pretrained(\"tiiuae/falcon-7b\")\\nmodel = AutoModelForCausalLM.from_pretrained(\\n    \"tiiuae/falcon-7b\",\\n    torch_dtype=torch.bfloat16,\\n    device_map=\"auto\",\\n    quantization_config=quantization_config,\\n)\\ninputs = tokenizer(\"In quantum physics, entanglement means\", return_tensors=\"pt\").to(\"cuda\")\\noutputs = model.generate(**inputs, max_new_tokens=100)\\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\\n\\nNotes', vector=FixedSizeList(dim=384))],\n",
              " 'search_results': [{'corpus_id': 8,\n",
              "   'score': np.float32(0.83425653),\n",
              "   'text': 'Choose a quantization method suitable for your hardware and use case (see the Overview or Selecting a quantization method guide to help you).\\nLoad a pre-quantized model from the Hugging Face Hub or load a float32/float16/bfloat16 model and apply a specific quantization method with [QuantizationConfig].\\n\\nThe example below demonstrates loading a 8B parameter model and quantizing it to 4-bits with bitsandbytes.\\nthon\\nimport torch\\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\\nmodel_id = \"meta-llama/Llama-3.1-8B-Instruct\"\\nquantization_config = BitsAndBytesConfig(\\n    load_in_4bit=True,\\n    bnb_4bit_compute_dtype=torch.bfloat16\\n)\\nmodel = AutoModelForCausalLM.from_pretrained(\\n    model_id,\\n    quantization_config=quantization_config,\\n    torch_dtype=torch.bfloat16,\\n    device_map=\"auto\"\\n)\\n\\nResources\\nTo explore quantization and related performance optimization concepts more deeply, check out the following resources.'},\n",
              "  {'corpus_id': 14,\n",
              "   'score': np.float32(0.8227661),\n",
              "   'text': 'Quantization reduces the memory burden of large models by representing the weights in a lower precision. Refer to the Quantization overview for more available quantization backends.\\nThe example below uses bitsandbytes to only quantize the weights to 4-bits.\\nthon\\nimport torch\\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\\nquantization_config = BitsAndBytesConfig(\\n    load_in_4bit=True,\\n    bnb_4bit_compute_dtype=torch.bfloat16,\\n    bnb_4bit_quant_type=\"nf4\",\\n    bnb_4bit_use_double_quant=True,\\n)\\ntokenizer = AutoTokenizer.from_pretrained(\"tiiuae/falcon-7b\")\\nmodel = AutoModelForCausalLM.from_pretrained(\\n    \"tiiuae/falcon-7b\",\\n    torch_dtype=torch.bfloat16,\\n    device_map=\"auto\",\\n    quantization_config=quantization_config,\\n)\\ninputs = tokenizer(\"In quantum physics, entanglement means\", return_tensors=\"pt\").to(\"cuda\")\\noutputs = model.generate(**inputs, max_new_tokens=100)\\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\\n\\nNotes'},\n",
              "  {'corpus_id': 7,\n",
              "   'score': np.float32(0.7815441),\n",
              "   'text': 'bitsandbytes\\nbitsandbytes features the LLM.int8 and QLoRA quantization to enable accessible large language model inference and training.\\nLLM.int8() is a quantization method that aims to make large language model inference more accessible without significant degradation. Unlike naive 8-bit quantization, which can result in loss of critical information and accuracy, LLM.int8() dynamically adapts to ensure sensitive components of the computation retain higher precision when needed.\\nQLoRA, or 4-bit quantization, compresses a model even further to 4-bits and inserts a small set of trainable low-rank adaptation (LoRA) weights to allowing training. \\n\\nNote: For a user-friendly quantization experience, you can use the bitsandbytes community space.\\n\\nRun the command below to install bitsandbytes.'},\n",
              "  {'corpus_id': 3,\n",
              "   'score': np.float32(0.6067128),\n",
              "   'text': 'Model optimization\\nQuantization using bitsandbytes\\nThe model can be loaded in 8 or 4 bits, greatly reducing the memory requirements while maintaining the performance of the original model. First make sure to install bitsandbytes, pip install bitsandbytes and make sure to have access to a GPU/accelerator that is supported by the library.\\n\\nbitsandbytes is being refactored to support multiple backends beyond CUDA. Currently, ROCm (AMD GPU) and Intel CPU implementations are mature, with Intel XPU in progress and Apple Silicon support expected by Q4/Q1. For installation instructions and the latest backend updates, visit this link.\\nWe value your feedback to help identify bugs before the full release! Check out these docs for more details and feedback links.'},\n",
              "  {'corpus_id': 6,\n",
              "   'score': np.float32(0.5944168),\n",
              "   'text': 'Model optimization\\nQuantization using Bitsandbytes\\nThe model can be loaded in 8 or 4 bits, greatly reducing the memory requirements while maintaining the performance of the original model. First make sure to install bitsandbytes, pip install bitsandbytes and to have access to a GPU/accelerator that is supported by the library.\\n\\nbitsandbytes is being refactored to support multiple backends beyond CUDA. Currently, ROCm (AMD GPU) and Intel CPU implementations are mature, with Intel XPU in progress and Apple Silicon support expected by Q4/Q1. For installation instructions and the latest backend updates, visit this link.\\nWe value your feedback to help identify bugs before the full release! Check out these docs for more details and feedback links.'}]}"
            ]
          },
          "metadata": {},
          "execution_count": 250
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's set up a particular reranker and run the whole pipeline on our favourite query,"
      ],
      "metadata": {
        "id": "slqeygZdCVJT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This might require you to restart a session\n",
        "!pip install -q mxbai-rerank"
      ],
      "metadata": {
        "id": "lmwLEd9wik_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from mxbai_rerank import MxbaiRerankV2\n",
        "\n",
        "reranker_model = MxbaiRerankV2(\"mixedbread-ai/mxbai-rerank-base-v2\")\n",
        "\n",
        "results = answer_with_rag(\"\"\"How to quantize a model in 4 bits?\"\"\",\n",
        "               client=client, model=model, reranker_model=reranker_model,\n",
        "               table=lance_table, verbose=True,\n",
        "               max_stage_1_results=15, max_results=5)\n",
        "print(results[\"answer\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OcZ_7hvFn3TV",
        "outputId": "ab78e794-2552-4ec6-ce2c-d5feba0e6732"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Based on the provided context, to quantize a model in 4 bits, you can use the\n",
            "`BitsAndBytesConfig` class from the Transformers library.\n",
            "\n",
            "Here's an example code snippet from the context:\n",
            "\n",
            "```python\n",
            "import torch\n",
            "from transformers import AutoTokenizer, AutoModelForCausalLM,\n",
            "BitsAndBytesConfig\n",
            "\n",
            "quantization_config = BitsAndBytesConfig(\n",
            "load_in_4bit=True,\n",
            "bnb_4bit_compute_dtype=torch.bfloat16,\n",
            "bnb_4bit_quant_type=\"nf4\",\n",
            "bnb_4bit_use_double_quant=True,\n",
            ")\n",
            "\n",
            "tokenizer = AutoTokenizer.from_pretrained(\"tiiuae/falcon-7b\")\n",
            "model = AutoModelForCausalLM.from_pretrained(\n",
            "\"tiiuae/falcon-7b\",\n",
            "torch_dtype=torch.bfloat16,\n",
            "device_map=\"auto\",\n",
            "quantization_config=quantization_config,\n",
            ")\n",
            "```\n",
            "\n",
            "In this example, the `BitsAndBytesConfig` class is used to configure the\n",
            "quantization of the model. The `load_in_4bit=True` parameter specifies that the\n",
            "model should be loaded in 4-bit precision. The `bnb_4bit_compute_dtype`\n",
            "parameter specifies the data type to use for 4-bit computations, and the\n",
            "`bnb_4bit_quant_type` and `bnb_4bit_use_double_quant` parameters are used to\n",
            "configure the quantization scheme.\n",
            "\n",
            "Note that this code snippet is from a different context, but it demonstrates\n",
            "how to use the `BitsAndBytesConfig` class to quantize a model in 4 bits.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5MD8fAQFoIVW",
        "outputId": "5ac3723d-b65b-4c01-86bc-a783e7e9d4c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'answer': 'Based on the provided context, to quantize a model in 4 bits, you can use the\\n`BitsAndBytesConfig` class from the Transformers library.\\n\\nHere\\'s an example code snippet from the context:\\n\\n```python\\nimport torch\\nfrom transformers import AutoTokenizer, AutoModelForCausalLM,\\nBitsAndBytesConfig\\n\\nquantization_config = BitsAndBytesConfig(\\nload_in_4bit=True,\\nbnb_4bit_compute_dtype=torch.bfloat16,\\nbnb_4bit_quant_type=\"nf4\",\\nbnb_4bit_use_double_quant=True,\\n)\\n\\ntokenizer = AutoTokenizer.from_pretrained(\"tiiuae/falcon-7b\")\\nmodel = AutoModelForCausalLM.from_pretrained(\\n\"tiiuae/falcon-7b\",\\ntorch_dtype=torch.bfloat16,\\ndevice_map=\"auto\",\\nquantization_config=quantization_config,\\n)\\n```\\n\\nIn this example, the `BitsAndBytesConfig` class is used to configure the\\nquantization of the model. The `load_in_4bit=True` parameter specifies that the\\nmodel should be loaded in 4-bit precision. The `bnb_4bit_compute_dtype`\\nparameter specifies the data type to use for 4-bit computations, and the\\n`bnb_4bit_quant_type` and `bnb_4bit_use_double_quant` parameters are used to\\nconfigure the quantization scheme.\\n\\nNote that this code snippet is from a different context, but it demonstrates\\nhow to use the `BitsAndBytesConfig` class to quantize a model in 4 bits.',\n",
              " 'stage_1_results': [BasicSchema(text=\"Quantizing a model is as simple as passing a quantization_config to the model. One can change the code snippet above with the changes below. We'll leverage the BitsAndyBytes quantization (but refer to this page for other quantization methods):\", vector=FixedSizeList(dim=384)),\n",
              "  BasicSchema(text='[!WARNING]\\n8 and 4-bit training is only supported for training extra parameters.\\n\\nCheck your memory footprint with get_memory_footprint.\\npy\\nprint(model.get_memory_footprint())\\nLoad quantized models with [~PreTrainedModel.from_pretrained] without a quantization_config.\\n\\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\\nmodel = AutoModelForCausalLM.from_pretrained(\"{your_username}/bloom-560m-8bit\", device_map=\"auto\")\\n\\nLLM.int8\\nThis section explores some of the specific features of 8-bit quantization, such as offloading, outlier thresholds, skipping module conversion, and finetuning.\\nOffloading\\n8-bit models can offload weights between the CPU and GPU to fit very large models into memory. The weights dispatched to the CPU are stored in float32 and aren\\'t converted to 8-bit. For example, enable offloading for bigscience/bloom-1b7 through [BitsAndBytesConfig].', vector=FixedSizeList(dim=384)),\n",
              "  BasicSchema(text='!pip install bitsandbytes\\nWe can then load models in 8-bit quantization by simply adding a load_in_8bit=True flag to from_pretrained.\\npython\\nmodel = AutoModelForCausalLM.from_pretrained(\"bigcode/octocoder\", load_in_8bit=True, pad_token_id=0)\\nNow, let\\'s run our example again and measure the memory usage.\\nthon\\npipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\\nresult = pipe(prompt, max_new_tokens=60)[0][\"generated_text\"][len(prompt):]\\nresult', vector=FixedSizeList(dim=384)),\n",
              "  BasicSchema(text='Model optimization\\nQuantization using bitsandbytes\\nThe model can be loaded in 8 or 4 bits, greatly reducing the memory requirements while maintaining the performance of the original model. First make sure to install bitsandbytes, pip install bitsandbytes and make sure to have access to a GPU/accelerator that is supported by the library.\\n\\nbitsandbytes is being refactored to support multiple backends beyond CUDA. Currently, ROCm (AMD GPU) and Intel CPU implementations are mature, with Intel XPU in progress and Apple Silicon support expected by Q4/Q1. For installation instructions and the latest backend updates, visit this link.\\nWe value your feedback to help identify bugs before the full release! Check out these docs for more details and feedback links.', vector=FixedSizeList(dim=384)),\n",
              "  BasicSchema(text='Learn how to quantize models in the Quantization guide.\\n\\nQuantoConfig\\n[[autodoc]] QuantoConfig\\nAqlmConfig\\n[[autodoc]] AqlmConfig\\nVptqConfig\\n[[autodoc]] VptqConfig\\nAwqConfig\\n[[autodoc]] AwqConfig\\nEetqConfig\\n[[autodoc]] EetqConfig\\nGPTQConfig\\n[[autodoc]] GPTQConfig\\nBitsAndBytesConfig\\n[[autodoc]] BitsAndBytesConfig\\nHfQuantizer\\n[[autodoc]] quantizers.base.HfQuantizer\\nHiggsConfig\\n[[autodoc]] HiggsConfig\\nHqqConfig\\n[[autodoc]] HqqConfig\\nFbgemmFp8Config\\n[[autodoc]] FbgemmFp8Config\\nCompressedTensorsConfig\\n[[autodoc]] CompressedTensorsConfig\\nTorchAoConfig\\n[[autodoc]] TorchAoConfig\\nBitNetConfig\\n[[autodoc]] BitNetConfig\\nSpQRConfig\\n[[autodoc]] SpQRConfig\\nFineGrainedFP8Config\\n[[autodoc]] FineGrainedFP8Config\\nQuarkConfig\\n[[autodoc]] QuarkConfig', vector=FixedSizeList(dim=384)),\n",
              "  BasicSchema(text='Model optimization\\nQuantization using Bitsandbytes\\nThe model can be loaded in 8 or 4 bits, greatly reducing the memory requirements while maintaining the performance of the original model. First make sure to install bitsandbytes, pip install bitsandbytes, and to have access to a GPU/accelerator that is supported by the library.\\n\\nbitsandbytes is being refactored to support multiple backends beyond CUDA. Currently, ROCm (AMD GPU) and Intel CPU implementations are mature, with Intel XPU in progress and Apple Silicon support expected by Q4/Q1. For installation instructions and the latest backend updates, visit this link.\\nWe value your feedback to help identify bugs before the full release! Check out these docs for more details and feedback links.', vector=FixedSizeList(dim=384)),\n",
              "  BasicSchema(text='Model optimization\\nQuantization using Bitsandbytes\\nThe model can be loaded in 8 or 4 bits, greatly reducing the memory requirements while maintaining the performance of the original model. First make sure to install bitsandbytes, pip install bitsandbytes and to have access to a GPU/accelerator that is supported by the library.\\n\\nbitsandbytes is being refactored to support multiple backends beyond CUDA. Currently, ROCm (AMD GPU) and Intel CPU implementations are mature, with Intel XPU in progress and Apple Silicon support expected by Q4/Q1. For installation instructions and the latest backend updates, visit this link.\\nWe value your feedback to help identify bugs before the full release! Check out these docs for more details and feedback links.', vector=FixedSizeList(dim=384)),\n",
              "  BasicSchema(text='bitsandbytes\\nbitsandbytes features the LLM.int8 and QLoRA quantization to enable accessible large language model inference and training.\\nLLM.int8() is a quantization method that aims to make large language model inference more accessible without significant degradation. Unlike naive 8-bit quantization, which can result in loss of critical information and accuracy, LLM.int8() dynamically adapts to ensure sensitive components of the computation retain higher precision when needed.\\nQLoRA, or 4-bit quantization, compresses a model even further to 4-bits and inserts a small set of trainable low-rank adaptation (LoRA) weights to allowing training. \\n\\nNote: For a user-friendly quantization experience, you can use the bitsandbytes community space.\\n\\nRun the command below to install bitsandbytes.', vector=FixedSizeList(dim=384)),\n",
              "  BasicSchema(text='Choose a quantization method suitable for your hardware and use case (see the Overview or Selecting a quantization method guide to help you).\\nLoad a pre-quantized model from the Hugging Face Hub or load a float32/float16/bfloat16 model and apply a specific quantization method with [QuantizationConfig].\\n\\nThe example below demonstrates loading a 8B parameter model and quantizing it to 4-bits with bitsandbytes.\\nthon\\nimport torch\\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\\nmodel_id = \"meta-llama/Llama-3.1-8B-Instruct\"\\nquantization_config = BitsAndBytesConfig(\\n    load_in_4bit=True,\\n    bnb_4bit_compute_dtype=torch.bfloat16\\n)\\nmodel = AutoModelForCausalLM.from_pretrained(\\n    model_id,\\n    quantization_config=quantization_config,\\n    torch_dtype=torch.bfloat16,\\n    device_map=\"auto\"\\n)\\n\\nResources\\nTo explore quantization and related performance optimization concepts more deeply, check out the following resources.', vector=FixedSizeList(dim=384)),\n",
              "  BasicSchema(text='echo -e \"# Function to calculate the factorial of a number\\\\ndef factorial(n):\" | transformers-cli run --task text-generation --model meta-llama/CodeLlama-7b-hf --device 0\\n\\nQuantization reduces the memory burden of large models by representing the weights in a lower precision. Refer to the Quantization overview for more available quantization backends.\\nThe example below uses bitsandbytes to only quantize the weights to 4-bits.', vector=FixedSizeList(dim=384)),\n",
              "  BasicSchema(text='Batched multi-image input and quantization with BitsAndBytes\\nThis implementation of the Mistral3 models supports batched text-images inputs with different number of images for each text.\\nThis example also how to use BitsAndBytes to load the model in 4bit quantization.\\nthon', vector=FixedSizeList(dim=384)),\n",
              "  BasicSchema(text='Quantize a model by creating a [HqqConfig] and specifying the nbits and group_size to replace for all the linear layers (torch.nn.Linear) of the model.\\n``` py\\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, HqqConfig\\nquant_config = HqqConfig(nbits=8, group_size=64)\\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\\n    \"meta-llama/Llama-3.1-8B\", \\n    torch_dtype=torch.float16, \\n    device_map=\"cuda\", \\n    quantization_config=quant_config\\n)', vector=FixedSizeList(dim=384)),\n",
              "  BasicSchema(text=\"Output:\\nHere is a Python function that transforms bytes to Giga bytes:\\\\n\\\\n\\\\ndef bytes_to_gigabytes(bytes):\\\\n    return bytes / 1024 / 1024 / 1024\\\\n\\\\n\\\\nThis function takes a single argument\\nWe're almost seeing the same output text as before - just the python is missing just before the code snippet. Let's see how much memory was required.\\npython\\nbytes_to_giga_bytes(torch.cuda.max_memory_allocated())\\nOutput:\\n9.543574333190918\\nJust 9.5GB! That's really not a lot for a >15 billion parameter model.\\nWhile we see very little degradation in accuracy for our model here, 4-bit quantization can in practice often lead to different results compared to 8-bit quantization or full bfloat16 inference. It is up to the user to try it out.\\nAlso note that inference here was again a bit slower compared to 8-bit quantization which is due to the more aggressive quantization method used for 4-bit quantization leading to \\\\( \\\\text{quantize} \\\\) and \\\\( \\\\text{dequantize} \\\\) taking longer during inference.\\npython\\ndel model\\ndel pipe\\npython\", vector=FixedSizeList(dim=384)),\n",
              "  BasicSchema(text='The model name is Meta-Llama-3.1-70B-Instruct.\\nThe number of centroids is given by 65536 (2^16).\\nThe number of residual centroids is given by 256 (2^8).\\n\\nThe equivalent bit-width calculation is given by the following.\\n\\nindex: log2(65536) = 16 / 8 = 2-bits\\nresidual index: log2(256) = 8 / 8 = 1-bit\\ntotal bit-width: 2 + 1 = 3-bits\\n\\nFrom here, estimate the model size by multiplying 70B * 3-bits / 8-bits/byte for a total of 26.25GB.\\nLoad a VPTQ quantized model with [~PreTrainedModel.from_pretrained].\\n\\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\\nquantized_model = AutoModelForCausalLM.from_pretrained(\\n    \"VPTQ-community/Meta-Llama-3.1-70B-Instruct-v16-k65536-65536-woft\",\\n    torch_dtype=\"auto\", \\n    device_map=\"auto\"\\n)', vector=FixedSizeList(dim=384)),\n",
              "  BasicSchema(text='Quantization reduces the memory burden of large models by representing the weights in a lower precision. Refer to the Quantization overview for more available quantization backends.\\nThe example below uses bitsandbytes to only quantize the weights to 4-bits.\\nthon\\nimport torch\\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\\nquantization_config = BitsAndBytesConfig(\\n    load_in_4bit=True,\\n    bnb_4bit_compute_dtype=torch.bfloat16,\\n    bnb_4bit_quant_type=\"nf4\",\\n    bnb_4bit_use_double_quant=True,\\n)\\ntokenizer = AutoTokenizer.from_pretrained(\"tiiuae/falcon-7b\")\\nmodel = AutoModelForCausalLM.from_pretrained(\\n    \"tiiuae/falcon-7b\",\\n    torch_dtype=torch.bfloat16,\\n    device_map=\"auto\",\\n    quantization_config=quantization_config,\\n)\\ninputs = tokenizer(\"In quantum physics, entanglement means\", return_tensors=\"pt\").to(\"cuda\")\\noutputs = model.generate(**inputs, max_new_tokens=100)\\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\\n\\nNotes', vector=FixedSizeList(dim=384))],\n",
              " 'search_results': [RankResult(index=4, score=-3.3299007415771484, document='Learn how to quantize models in the Quantization guide.\\n\\nQuantoConfig\\n[[autodoc]] QuantoConfig\\nAqlmConfig\\n[[autodoc]] AqlmConfig\\nVptqConfig\\n[[autodoc]] VptqConfig\\nAwqConfig\\n[[autodoc]] AwqConfig\\nEetqConfig\\n[[autodoc]] EetqConfig\\nGPTQConfig\\n[[autodoc]] GPTQConfig\\nBitsAndBytesConfig\\n[[autodoc]] BitsAndBytesConfig\\nHfQuantizer\\n[[autodoc]] quantizers.base.HfQuantizer\\nHiggsConfig\\n[[autodoc]] HiggsConfig\\nHqqConfig\\n[[autodoc]] HqqConfig\\nFbgemmFp8Config\\n[[autodoc]] FbgemmFp8Config\\nCompressedTensorsConfig\\n[[autodoc]] CompressedTensorsConfig\\nTorchAoConfig\\n[[autodoc]] TorchAoConfig\\nBitNetConfig\\n[[autodoc]] BitNetConfig\\nSpQRConfig\\n[[autodoc]] SpQRConfig\\nFineGrainedFP8Config\\n[[autodoc]] FineGrainedFP8Config\\nQuarkConfig\\n[[autodoc]] QuarkConfig'),\n",
              "  RankResult(index=13, score=-4.148468971252441, document='The model name is Meta-Llama-3.1-70B-Instruct.\\nThe number of centroids is given by 65536 (2^16).\\nThe number of residual centroids is given by 256 (2^8).\\n\\nThe equivalent bit-width calculation is given by the following.\\n\\nindex: log2(65536) = 16 / 8 = 2-bits\\nresidual index: log2(256) = 8 / 8 = 1-bit\\ntotal bit-width: 2 + 1 = 3-bits\\n\\nFrom here, estimate the model size by multiplying 70B * 3-bits / 8-bits/byte for a total of 26.25GB.\\nLoad a VPTQ quantized model with [~PreTrainedModel.from_pretrained].\\n\\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\\nquantized_model = AutoModelForCausalLM.from_pretrained(\\n    \"VPTQ-community/Meta-Llama-3.1-70B-Instruct-v16-k65536-65536-woft\",\\n    torch_dtype=\"auto\", \\n    device_map=\"auto\"\\n)'),\n",
              "  RankResult(index=14, score=-4.77755880355835, document='Quantization reduces the memory burden of large models by representing the weights in a lower precision. Refer to the Quantization overview for more available quantization backends.\\nThe example below uses bitsandbytes to only quantize the weights to 4-bits.\\nthon\\nimport torch\\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\\nquantization_config = BitsAndBytesConfig(\\n    load_in_4bit=True,\\n    bnb_4bit_compute_dtype=torch.bfloat16,\\n    bnb_4bit_quant_type=\"nf4\",\\n    bnb_4bit_use_double_quant=True,\\n)\\ntokenizer = AutoTokenizer.from_pretrained(\"tiiuae/falcon-7b\")\\nmodel = AutoModelForCausalLM.from_pretrained(\\n    \"tiiuae/falcon-7b\",\\n    torch_dtype=torch.bfloat16,\\n    device_map=\"auto\",\\n    quantization_config=quantization_config,\\n)\\ninputs = tokenizer(\"In quantum physics, entanglement means\", return_tensors=\"pt\").to(\"cuda\")\\noutputs = model.generate(**inputs, max_new_tokens=100)\\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\\n\\nNotes'),\n",
              "  RankResult(index=11, score=-5.344222068786621, document='Quantize a model by creating a [HqqConfig] and specifying the nbits and group_size to replace for all the linear layers (torch.nn.Linear) of the model.\\n``` py\\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, HqqConfig\\nquant_config = HqqConfig(nbits=8, group_size=64)\\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\\n    \"meta-llama/Llama-3.1-8B\", \\n    torch_dtype=torch.float16, \\n    device_map=\"cuda\", \\n    quantization_config=quant_config\\n)'),\n",
              "  RankResult(index=1, score=-5.445117473602295, document='[!WARNING]\\n8 and 4-bit training is only supported for training extra parameters.\\n\\nCheck your memory footprint with get_memory_footprint.\\npy\\nprint(model.get_memory_footprint())\\nLoad quantized models with [~PreTrainedModel.from_pretrained] without a quantization_config.\\n\\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\\nmodel = AutoModelForCausalLM.from_pretrained(\"{your_username}/bloom-560m-8bit\", device_map=\"auto\")\\n\\nLLM.int8\\nThis section explores some of the specific features of 8-bit quantization, such as offloading, outlier thresholds, skipping module conversion, and finetuning.\\nOffloading\\n8-bit models can offload weights between the CPU and GPU to fit very large models into memory. The weights dispatched to the CPU are stored in float32 and aren\\'t converted to 8-bit. For example, enable offloading for bigscience/bloom-1b7 through [BitsAndBytesConfig].')]}"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    }
  ]
}
