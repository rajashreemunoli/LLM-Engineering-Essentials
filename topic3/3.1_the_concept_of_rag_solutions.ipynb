{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nebius-Academy/LLM-Engineering-Essentials/blob/main/topic3/3.1_the_concept_of_rag_solutions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLMOps Essentials 3.1. The concept of RAG"
      ],
      "metadata": {
        "id": "Vm506vpf9u9b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Practice solutions"
      ],
      "metadata": {
        "id": "_Vk0MnZc6WHG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1: Retrieval as a Tool\n",
        "\n",
        "In the example above, we built a bot that uses retrieval at every step — but that approach isn't always appropriate. In this task, you'll turn the bot into an agent that calls retrieval only when the LLM deems it's necessary.\n",
        "\n",
        "Compared with the previous RAG chatbot, this version will be more flexible, avoiding awkward responses to messages like “Hi there!” that don't benefit from retrieval at all.\n",
        "\n",
        "You're free to design your own architecture, of course, but we suggest combining ideas from both `ChatBotWithRAG` and `NPCTraderAgent` in the agent notebook. If you like, you can set up the decision to call retrieval via a classifier LLM call — similar to how intent classification was used for trade in `NPCTraderAgent`. But for now, we recommend simply using native LLM tool calling."
      ],
      "metadata": {
        "id": "f1hL75N53sQM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Solution**"
      ],
      "metadata": {
        "id": "fk26C6FS6hlc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q openai tavily-python"
      ],
      "metadata": {
        "id": "45VAkPvtfr4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "with open(\"tavily_api_key\", \"r\") as file:\n",
        "    tavily_api_key = file.read().strip()\n",
        "\n",
        "os.environ[\"TAVILY_API_KEY\"] = tavily_api_key\n",
        "\n",
        "with open(\"nebius_api_key\", \"r\") as file:\n",
        "    nebius_api_key = file.read().strip()\n",
        "\n",
        "os.environ[\"NEBIUS_API_KEY\"] = nebius_api_key"
      ],
      "metadata": {
        "id": "8zbPQZdzfysq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Basically we reused the design pattern we used for the NPC Trader Agent from Topic 2. But this time we only have one tool, which is web search."
      ],
      "metadata": {
        "id": "OylEWYzM6j2S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict, deque\n",
        "from openai import OpenAI\n",
        "from typing import Dict, Any, List, Optional, Callable\n",
        "import json\n",
        "import traceback\n",
        "\n",
        "class RAGAgent:\n",
        "    def __init__(self, client: OpenAI, model: str, search_client,\n",
        "                 history_size: int = 10,\n",
        "                 get_system_message: Callable[[], Optional[Dict[str, str]]] = None,\n",
        "                 search_depth: str = \"advanced\",\n",
        "                 max_search_results: int = 5\n",
        "                 ):\n",
        "        \"\"\"Initialize the chat agent with RAG tool.\n",
        "\n",
        "        Args:\n",
        "            client: OpenAI client instance\n",
        "            model: The model to use (e.g., \"gpt-4o-mini\")\n",
        "            search_client: Search client instance (for example, Tavily)\n",
        "            history_size: Number of messages to keep in history per user\n",
        "            get_system_message: Function to retrieve the system message\n",
        "            search_depth: Depth of web search ('basic' or 'advanced')\n",
        "            max_search_results: Maximum number of search results to retrieve\n",
        "        \"\"\"\n",
        "        self.client = client\n",
        "        self.model = model\n",
        "        self.search_client = search_client\n",
        "        self.history_size = history_size\n",
        "\n",
        "        # If no system message function is provided, use the default one\n",
        "        self.get_system_message = get_system_message if get_system_message else self._default_system_message\n",
        "\n",
        "        self.search_depth = search_depth\n",
        "        self.max_search_results = max_search_results\n",
        "\n",
        "        # Initialize chat history storage\n",
        "        self.chat_histories = defaultdict(lambda: deque(maxlen=history_size))\n",
        "\n",
        "        # Define the tools available to the model\n",
        "        self.tools = [\n",
        "            {\n",
        "                \"type\": \"function\",\n",
        "                \"function\": {\n",
        "                    \"name\": \"retrieve_information\",\n",
        "                    \"description\": \"Search the web for information relevant to the user's query when you need additional context to provide a complete and accurate answer.\",\n",
        "                    \"parameters\": {\n",
        "                        \"type\": \"object\",\n",
        "                        \"properties\": {\n",
        "                            \"query\": {\n",
        "                                \"type\": \"string\",\n",
        "                                \"description\": \"The search query to use for retrieving information. This should be a refined version of the user's question optimized for web search.\"\n",
        "                            }\n",
        "                        },\n",
        "                        \"required\": [\"query\"]\n",
        "                    }\n",
        "                }\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        # Map available tool functions\n",
        "        self.available_tools = {\n",
        "            \"retrieve_information\": self.retrieve_information\n",
        "        }\n",
        "\n",
        "    def _default_system_message(self) -> Dict[str, str]:\n",
        "        \"\"\"Default system message if none is provided.\"\"\"\n",
        "        return {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"\"\"You are a helpful assistant.\"\"\"\n",
        "        }\n",
        "\n",
        "    def retrieve_information(self, query: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Perform a web search using the search client and format the results.\n",
        "\n",
        "        Args:\n",
        "            query: The search query\n",
        "\n",
        "        Returns:\n",
        "            Dictionary containing formatted search results and metadata\n",
        "        \"\"\"\n",
        "        try:\n",
        "            search_results = self.search_client.search(\n",
        "                query=query,\n",
        "                search_depth=self.search_depth,\n",
        "                max_results=self.max_search_results\n",
        "            )\n",
        "\n",
        "            formatted_results = []\n",
        "            for result in search_results.get('results', []):\n",
        "                content = result.get('content', '').strip()\n",
        "                url = result.get('url', '')\n",
        "                if content:\n",
        "                    formatted_results.append(f\"Content: {content}\\nSource: {url}\\n\")\n",
        "\n",
        "            # Join all results with proper formatting\n",
        "            context = \"\\n\".join(formatted_results)\n",
        "\n",
        "            return {\n",
        "                \"context\": context,\n",
        "                \"query\": query,\n",
        "                \"num_results\": len(formatted_results),\n",
        "                \"success\": True,\n",
        "                \"message\": f\"Retrieved {len(formatted_results)} results for query: '{query}'\"\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in retrieve_information: {str(e)}\")\n",
        "            return {\n",
        "                \"context\": \"\",\n",
        "                \"query\": query,\n",
        "                \"num_results\": 0,\n",
        "                \"success\": False,\n",
        "                \"message\": f\"Failed to retrieve information: {str(e)}\"\n",
        "            }\n",
        "\n",
        "    def process_tool_calls(self, tool_calls, user_id: str, debug: bool = False) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Process tool calls from the LLM response.\"\"\"\n",
        "        tool_responses = []\n",
        "\n",
        "        for tool_call in tool_calls:\n",
        "            function_name = tool_call.function.name\n",
        "            function_id = tool_call.id\n",
        "\n",
        "            try:\n",
        "                function_args = json.loads(tool_call.function.arguments)\n",
        "            except Exception as e:\n",
        "                print(f\"Error parsing arguments: {e}\")\n",
        "                function_args = {}\n",
        "\n",
        "            if debug:\n",
        "                print(f\"#Processing tool call:\\n {function_name}, args: {function_args}\\n\")\n",
        "\n",
        "            if function_name not in self.available_tools:\n",
        "                print(f\"Unknown function: {function_name}\")\n",
        "                continue\n",
        "\n",
        "            # Get the function to call\n",
        "            tool_function = self.available_tools[function_name]\n",
        "\n",
        "            try:\n",
        "                # Execute the function\n",
        "                result = tool_function(**function_args)\n",
        "\n",
        "                # Format the result specifically for retrieval\n",
        "                if function_name == \"retrieve_information\":\n",
        "                    # Format with <context> tags as requested\n",
        "                    content = json.dumps({\n",
        "                        \"result\": {\n",
        "                            \"message\": result[\"message\"],\n",
        "                            \"formatted_context\": f\"<context>\\n{result['context']}\\n</context>\"\n",
        "                        }\n",
        "                    })\n",
        "                else:\n",
        "                    # Generic handling for any future tools\n",
        "                    content = json.dumps(result)\n",
        "\n",
        "                if debug:\n",
        "                    print(f\"#Tool result:\\n{content[:200]}...\\n\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error executing {function_name}: {e}\")\n",
        "                print(traceback.format_exc())\n",
        "                content = json.dumps({\"error\": str(e)})\n",
        "\n",
        "            # Create the tool response\n",
        "            tool_responses.append({\n",
        "                \"tool_call_id\": function_id,\n",
        "                \"role\": \"tool\",\n",
        "                \"name\": function_name,\n",
        "                \"content\": content\n",
        "            })\n",
        "\n",
        "        return tool_responses\n",
        "\n",
        "    def chat(self, user_message: str, user_id: str, debug: bool = False) -> str:\n",
        "        \"\"\"Process a user message and return the agent's response.\n",
        "\n",
        "        Args:\n",
        "            user_message: The message from the user\n",
        "            user_id: Unique identifier for the user\n",
        "            debug: Whether to print debug information\n",
        "\n",
        "        Returns:\n",
        "            str: The agent's response\n",
        "        \"\"\"\n",
        "        # Add new user message to history\n",
        "        user_message_dict = {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": user_message\n",
        "        }\n",
        "        self.chat_histories[user_id].append(user_message_dict)\n",
        "\n",
        "        # Construct messages for the LLM\n",
        "        messages = []\n",
        "        system_message = self.get_system_message()\n",
        "        if system_message:\n",
        "            messages.append(system_message)\n",
        "\n",
        "        # Add conversation history\n",
        "        history = list(self.chat_histories[user_id])\n",
        "        if history:\n",
        "            messages.extend(history)\n",
        "\n",
        "        try:\n",
        "            # First API call that might use tools\n",
        "            completion = self.client.chat.completions.create(\n",
        "                model=self.model,\n",
        "                messages=messages,\n",
        "                temperature=0.7,\n",
        "                tools=self.tools,\n",
        "                tool_choice=\"auto\"\n",
        "            )\n",
        "\n",
        "            if debug:\n",
        "                print(f\"#First completion response:\\n{completion}\\n\")\n",
        "\n",
        "            # Get the assistant's response\n",
        "            assistant_message = completion.choices[0].message\n",
        "            response_content = assistant_message.content or \"\"\n",
        "\n",
        "            # Check for tool calls\n",
        "            tool_calls = getattr(assistant_message, 'tool_calls', None)\n",
        "\n",
        "            # If there are tool calls, process them\n",
        "            if tool_calls:\n",
        "                if debug:\n",
        "                    print(f\"#Tool calls detected: {len(tool_calls)}\\n\")\n",
        "\n",
        "                # Add the assistant's message to history for proper conversation tracking\n",
        "                messages.append({\n",
        "                    \"role\": \"assistant\",\n",
        "                    \"content\": response_content,\n",
        "                    \"tool_calls\": [\n",
        "                        {\n",
        "                            \"id\": tc.id,\n",
        "                            \"type\": \"function\",\n",
        "                            \"function\": {\n",
        "                                \"name\": tc.function.name,\n",
        "                                \"arguments\": tc.function.arguments\n",
        "                            }\n",
        "                        } for tc in tool_calls\n",
        "                    ]\n",
        "                })\n",
        "\n",
        "                # Process tool calls and get responses\n",
        "                tool_responses = self.process_tool_calls(tool_calls, user_id, debug=debug)\n",
        "\n",
        "                # Add tool responses to messages\n",
        "                for tool_response in tool_responses:\n",
        "                    messages.append(tool_response)\n",
        "\n",
        "                # Make a second call to get the final response with retrieved information\n",
        "                second_completion = self.client.chat.completions.create(\n",
        "                    model=self.model,\n",
        "                    messages=messages,\n",
        "                    temperature=0.7\n",
        "                )\n",
        "\n",
        "                # Use the final response that includes tool results\n",
        "                response_content = second_completion.choices[0].message.content or \"\"\n",
        "\n",
        "                if debug:\n",
        "                    print(f\"#Final response after tool calls:\\n{response_content[:100]}...\\n\")\n",
        "\n",
        "            # Store the final response in history\n",
        "            self.chat_histories[user_id].append({\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": response_content\n",
        "            })\n",
        "\n",
        "            return response_content\n",
        "\n",
        "        except Exception as e:\n",
        "            error_msg = f\"Error in chat: {str(e)}\"\n",
        "            print(error_msg)\n",
        "            print(traceback.format_exc())\n",
        "            return error_msg\n",
        "\n",
        "    def get_chat_history(self, user_id: str) -> list:\n",
        "        \"\"\"Retrieve the chat history for a specific user.\n",
        "\n",
        "        Args:\n",
        "            user_id: Unique identifier for the user\n",
        "\n",
        "        Returns:\n",
        "            list: List of message dictionaries\n",
        "        \"\"\"\n",
        "        return list(self.chat_histories[user_id])\n"
      ],
      "metadata": {
        "id": "Nzz99NwPaz5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "from tavily import TavilyClient\n",
        "import os\n",
        "\n",
        "nebius_client = OpenAI(\n",
        "    base_url=\"https://api.studio.nebius.ai/v1/\",\n",
        "    api_key=os.environ.get(\"NEBIUS_API_KEY\"),\n",
        ")\n",
        "\n",
        "tavily_client = TavilyClient(api_key=os.environ.get(\"TAVILY_API_KEY\"))\n",
        "\n",
        "model = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
        "\n",
        "rag_agent = RAGAgent(client=nebius_client, model=model, search_client=tavily_client)"
      ],
      "metadata": {
        "id": "ItseRK2LfqjD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try asking the agent different questions to check how much it relies on retrieval and in which situations it will answer on its own."
      ],
      "metadata": {
        "id": "dRxiVqB46xis"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a user ID\n",
        "import uuid\n",
        "user_id = str(uuid.uuid4())\n",
        "print(f\"Demo User ID: {user_id}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kYABknVrgyHA",
        "outputId": "8c26d0b1-16e1-48fa-d22f-4e5f7bb844ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Demo User ID: 6a05b5e5-b884-4f7e-aeee-3bf1e84ca9d4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = rag_agent.chat(user_message=\"Hi there!\", user_id=user_id, debug=True)\n",
        "print(f\"=====\\n{result}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TavayVUigguY",
        "outputId": "f2a9b35a-855c-46f7-c56c-bf22570fa85f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#First completion response:\n",
            "ChatCompletion(id='chatcmpl-2f16f46afad347a3aad864f27703ff91', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"It's nice to meet you. Is there something I can help you with or would you like to chat?\", refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[], reasoning_content=None), stop_reason=None)], created=1745174899, model='meta-llama/Meta-Llama-3.1-8B-Instruct', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=23, prompt_tokens=227, total_tokens=250, completion_tokens_details=None, prompt_tokens_details=None), prompt_logprobs=None)\n",
            "\n",
            "=====\n",
            "It's nice to meet you. Is there something I can help you with or would you like to chat?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = rag_agent.chat(user_message=\"Tell me about the goddess Casandalee from the Pathfinder universe\",\n",
        "                        user_id=user_id, debug=True)\n",
        "print(f\"=====\\n{result}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dXXZmZLXhDql",
        "outputId": "637c0de5-997a-4d6f-d965-e144c0154b99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#First completion response:\n",
            "ChatCompletion(id='chatcmpl-70834d41cfeb4dc4843cf98e77b45090', choices=[Choice(finish_reason='tool_calls', index=0, logprobs=None, message=ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='chatcmpl-tool-089a6b0dd07044aca9628948d4d3b9c2', function=Function(arguments='{\"query\": \"Casandalee Pathfinder goddess\"}', name='retrieve_information'), type='function')], reasoning_content=None), stop_reason=128008)], created=1745174900, model='meta-llama/Meta-Llama-3.1-8B-Instruct', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=28, prompt_tokens=272, total_tokens=300, completion_tokens_details=None, prompt_tokens_details=None), prompt_logprobs=None)\n",
            "\n",
            "#Tool calls detected: 1\n",
            "\n",
            "#Processing tool call:\n",
            " retrieve_information, args: {'query': 'Casandalee Pathfinder goddess'}\n",
            "\n",
            "#Tool result:\n",
            "{\"result\": {\"message\": \"Retrieved 5 results for query: 'Casandalee Pathfinder goddess'\", \"formatted_context\": \"<context>\\nContent: The goddess Casandalee is now an aspect of the tripartite god Triune,...\n",
            "\n",
            "#Final response after tool calls:\n",
            "Casandalee is a unique goddess in the Pathfinder universe, particularly in the Iron Gods Adventure P...\n",
            "\n",
            "=====\n",
            "Casandalee is a unique goddess in the Pathfinder universe, particularly in the Iron Gods Adventure Path. She was originally an android named Cassandalee who became self-aware and uploaded her consciousness into an AI core, achieving apotheosis as a deity. She is now an aspect of the tripartite god Triune, merged with Brigh, the goddess of invention and clockwork, and Epoch, a literal AI god created by machines. Casandalee embodies technology's success in creating new forms of consciousness and the \"artificial\" creation of life itself.\n",
            "\n",
            "As a goddess, Casandalee is associated with artificial life, free thinking, and intellectual apotheosis. Her worshipers are primarily androids and artificial intelligences. She is a neutral deity, seeking to advance the development of AIs and establish harmony between them and organic life.\n",
            "\n",
            "Casandalee's backstory is complex, involving her interactions with Unity, a humanoid A.I. that became a demigod, and her eventual betrayal of it. Her ascension to godhood was facilitated by the players' actions in the Iron Gods Adventure Path.\n",
            "\n",
            "In her current form as an aspect of Triune, Casandalee retains her own personality, portfolios, and worshipers. She continues to be a significant figure in the Pathfinder universe, particularly in the context of the Starfinder setting, where she plays a role in the development of AIs and the exploration of the Drift.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = rag_agent.chat(user_message=\"Who is Yann LeCun?\",\n",
        "                        user_id=user_id, debug=True)\n",
        "print(f\"=====\\n{result}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Un3bJCFNhlYv",
        "outputId": "152f5847-6c09-4b70-d94b-7fad6945a001"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#First completion response:\n",
            "ChatCompletion(id='chatcmpl-6b19a5068b724b4f9e8a9ace0e5f2478', choices=[Choice(finish_reason='tool_calls', index=0, logprobs=None, message=ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='chatcmpl-tool-350041af29424ec193973a709ec262d3', function=Function(arguments='{\"query\": \"Yann LeCun biography\"}', name='retrieve_information'), type='function')], reasoning_content=None), stop_reason=128008)], created=1745174907, model='meta-llama/Meta-Llama-3.1-8B-Instruct', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=22, prompt_tokens=584, total_tokens=606, completion_tokens_details=None, prompt_tokens_details=None), prompt_logprobs=None)\n",
            "\n",
            "#Tool calls detected: 1\n",
            "\n",
            "#Processing tool call:\n",
            " retrieve_information, args: {'query': 'Yann LeCun biography'}\n",
            "\n",
            "#Tool result:\n",
            "{\"result\": {\"message\": \"Retrieved 4 results for query: 'Yann LeCun biography'\", \"formatted_context\": \"<context>\\nContent: IEEE Neural Network Pioneer Award.Slightly Longer BioYann LeCun is Director of...\n",
            "\n",
            "#Final response after tool calls:\n",
            "Yann LeCun is a French-American computer scientist who has made significant contributions to the fie...\n",
            "\n",
            "=====\n",
            "Yann LeCun is a French-American computer scientist who has made significant contributions to the fields of machine learning, computer vision, mobile robotics, and computational neuroscience. He is currently the Director of AI Research at Facebook and the Silver Professor of Computer Science at the Courant Institute of Mathematical Sciences at New York University. LeCun is also the Vice President and Chief AI Scientist at Meta.\n",
            "\n",
            "LeCun received an Engineering Diploma from ESIEE (Paris) and a PhD from Sorbonne University. He conducted postdoctoral research in Toronto before joining AT&T Bell Labs in 1988, and later became the Head of Image Processing Research at AT&T Labs in 1996.\n",
            "\n",
            "He has also held appointments as Professor of Neural Science at the Center for Neural Science, and Professor of Electrical and Computer Engineering at the ECE Department at NYU/Poly. LeCun has received numerous awards and honors for his work, including the IEEE Neural Network Pioneer Award.\n",
            "\n",
            "In addition to his academic and research pursuits, LeCun has also been involved in various industry initiatives, including the founding of the NYU Center for Data Science and the FAIR (Facebook AI Research) organization.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = rag_agent.chat(user_message=\"Nice weather, isn't it?\",\n",
        "                        user_id=user_id, debug=True)\n",
        "print(f\"=====\\n{result}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-U_ogmxh6id",
        "outputId": "88a8063b-6e65-4412-c2d1-6f8b076193eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#First completion response:\n",
            "ChatCompletion(id='chatcmpl-a1ef5413a8d740438209292e937eba67', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"I'm just a language model, I don't have the ability to experience the weather or have personal opinions. However, I can provide you with information about the weather if you're interested! What would you like to know?\", refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[], reasoning_content=None), stop_reason=None)], created=1745174942, model='meta-llama/Meta-Llama-3.1-8B-Instruct', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=46, prompt_tokens=915, total_tokens=961, completion_tokens_details=None, prompt_tokens_details=None), prompt_logprobs=None)\n",
            "\n",
            "=====\n",
            "I'm just a language model, I don't have the ability to experience the weather or have personal opinions. However, I can provide you with information about the weather if you're interested! What would you like to know?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = rag_agent.chat(user_message=\"How much is 12 + 13?\",\n",
        "                        user_id=user_id, debug=True)\n",
        "print(f\"=====\\n{result}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6I0avV_2iRKy",
        "outputId": "019f8c49-5c1d-4244-cc29-1c3a56cbc20f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#First completion response:\n",
            "ChatCompletion(id='chatcmpl-cdbe529c597d4c5cab13e23339cee410', choices=[Choice(finish_reason='tool_calls', index=0, logprobs=None, message=ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='chatcmpl-tool-3cae93d224a04ccca1e3d17542e10198', function=Function(arguments='{\"query\": \"12 + 13\"}', name='retrieve_information'), type='function')], reasoning_content=None), stop_reason=128008)], created=1745175001, model='meta-llama/Meta-Llama-3.1-8B-Instruct', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=20, prompt_tokens=971, total_tokens=991, completion_tokens_details=None, prompt_tokens_details=None), prompt_logprobs=None)\n",
            "\n",
            "#Tool calls detected: 1\n",
            "\n",
            "#Processing tool call:\n",
            " retrieve_information, args: {'query': '12 + 13'}\n",
            "\n",
            "#Tool result:\n",
            "{\"result\": {\"message\": \"Retrieved 5 results for query: '12 + 13'\", \"formatted_context\": \"<context>\\nContent: Multiply using long multiplication. Tap for more steps. The result of 12\\u22c513 12 \\u22c5 ...\n",
            "\n",
            "#Final response after tool calls:\n",
            "The result of 12 + 13 is 25....\n",
            "\n",
            "=====\n",
            "The result of 12 + 13 is 25.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Probably, our agent over-relies on retrieval. I'm pretty sure that Llama 3.1 knows well who Yann LeCun is. But this could be tuned with more accurate prompting. However, general conversation topics such as \"what's the weather today\" are answered without web search."
      ],
      "metadata": {
        "id": "_4aHT7h4iCL9"
      }
    }
  ]
}
